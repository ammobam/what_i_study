{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210818 RNN, 자연어 처리_Pytorch이용.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7A3wc4nPh1guOS+qd0qYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammobam/Study_DeepLearing/blob/main/210818_RNN%2C_%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC_Pytorch%EC%9D%B4%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emLpe12XpnaP"
      },
      "source": [
        "# ** RNN\n",
        "- 시계열 예측, 자연어 처리\n",
        "- 과거의 데이터로 미래를 '예측'하기 위함\n",
        "\n",
        "- cf) CNN : 이미지 처리\n",
        "- 작은 패턴으로 분할하여 전체를 자세히 관찰하기 위함\n",
        "\n",
        "## 1. RNN\n",
        "- Recurrent Neural Network (순환신경망)\n",
        "\n",
        "### (1) RNN의 이해\n",
        "\t- TRIANGEL, INTEGRAL\n",
        "\t- 단어를 구성하는 단어는 동일하지만 순서가 다르면 다른 단어가 됨\n",
        "- 순서를 따르는 패턴을 찾아서 상관관계, 인과관계를 찾고자 RNN이 개발됨\n",
        "- 일반적인 인공신경망\n",
        "\t- 입력층(입력) > 은닉층(연산) > 은닉층(연산) > 출력층(출력)\n",
        "- RNN\n",
        "\t- 입력층(입력) > 은닉층(연산) > 은닉층(연산) > 출력층(출력)\n",
        "\t       ┃└─────────────┘            ┃\n",
        "\t       └─────────────────────┘\n",
        "\t- 은닉층에서 연산할 때 이전에 수행한 연산 + 현재연산의 조합으로 연산을 수행함\n",
        "\n",
        "### (2) RNN의 과정\n",
        "- timestep = 0\n",
        "\t- 어떤 초기값과 계산을 시작함\n",
        "\t- 은닉층 값을 계산하고 결과값이 나오면 다음 은닉층으로 전송\n",
        "\t- 현재 계산값을 보존하고 있다가\n",
        "- 다음 계산을 수행하는 timestep=1 시점에서\n",
        "\t- 현재의 입력과 이전에 계산한 값으로 다시 결과를 생성하여 다음 은닉층에 전달함\n",
        "- 이 과정을 전체 timestep에 지정한 만큼 반복하게 됨\n",
        "\n",
        "### (3) RNN의 역전파\n",
        "- BPTT (Back Propagation Through Time)\n",
        "- t=0, t=3까지 연산을 수행한 경우, 현재 시점까지 전체에 대하여 역전파를 수행해야 함\n",
        "\n",
        "- 예시\n",
        "\t- (단어 pytorch) 알파벳을 한 개 씩 입력 받아서 다음 알파벳을 예측하는 경우,\n",
        "\t\t- p 입력 > y 예측 > py를 입력 > t 예측\n",
        "\t\t- t가 아닌 글자를 예측했다면 잘못된 예측을 수행했으므로 손실을 계산하여 예측함\n",
        "\t- DNN, CNN 구조\n",
        "\t\t- 이때 y에게만 영향을 줌\n",
        "\t- RNN 구조\n",
        "\t\t- 이때 y와 그 이전이 입력값인 p에게도 역전파를 수행함\n",
        "\n",
        "\n",
        "## 2. RNN의 활성화 함수\n",
        "- tanh (하이퍼볼릭 탄젠트 함수) 이용\n",
        "\n",
        "## 3. 실습\n",
        "- 글자 또는 단어를 입력하면 문장을 생성하는 입력기\n",
        "    - (1) 일반적인 신경망 이용\n",
        "    - (2) RNN 이용\n",
        "\n",
        "## 4. RNN의 문제점\n",
        "- 입력이 커지면 기울기 소실 문제가 발생함\n",
        "\t- 입력이 커지는 경우\n",
        "\t \t- 문장의 길이가 길어지거나\n",
        "\t \t- 예측하기 위해 훈련하는 데이터의 길이가 길어짐\n",
        "- tanh()\n",
        "\t- tanh는 미분하게 되면 0에서 1 사이의 값이 나옴\n",
        "\t- 이를 여러 번 곱하면 기울기가 0으로 수렴할 가능성이 높음\n",
        "- 해결책\n",
        "\t- (1) 활성화 함수를 변경함\n",
        "\t- (2) LSTM, GRU 모델 이용\n",
        "\t\t- LSTM, GRU : 이전 은닉층을 기억하여 사용하는 방식\n",
        "\t\t- 미분을 이용하지 않고, 이전의 모든 층을 이용함\n",
        "\t\t- LSTM : 이전 층의 셀 상태를 기억함\n",
        "\t\t- GRU : 이전층을 단순화하여 RNN와 비슷하게 만들고 LSTM의 성능을 나타내도록 한 것\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK5bfY0Dptz3"
      },
      "source": [
        "## 일반적인 신경망과 RNN 모델의 차이"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2nMX_npu3v"
      },
      "source": [
        "### 일반적인 신경망"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmasgyjNjypj"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fXL6Q7Lqc1W"
      },
      "source": [
        "#### 하이퍼 파라미터\n",
        "- 학습률\n",
        "    - 경사하강법에서 보폭에 해당함\n",
        "    - 전체 최저점을 못 찾는 경우, 학습률 줄이기\n",
        "    - 과대적합의 경우, 학습률 늘리기\n",
        "    - 학습 시간을 단축하고자 하는 경우, 학습률 늘리기\n",
        "- 반복횟수\n",
        "    - 모델 생성을 위한 훈련 횟수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LukpTwrxp-fE"
      },
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "n_hidden = 35    # 은닉\n",
        "lr = 0.01        # 학습률\n",
        "epochs = 1000    # 반복횟수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cjGqzR_rcCW"
      },
      "source": [
        "#### 샘플데이터 / 토큰 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDvMBIVHp-jE"
      },
      "source": [
        "# 샘플 데이터와 분류를 위한 토큰 생성\n",
        "\n",
        "# 샘플 데이터\n",
        "string = 'hello, pytorch. How long can a RNN cell remember? show me yout limit!'\n",
        "\n",
        "# 문장에 포함된 글자들. 0과 1은 시작과 종료를 위한 문장\n",
        "## 스페이스도 잊지 말고 넣어주자.\n",
        "chars = 'abcdefghijklmnopqrstuvwxyz ?!,.01'\n",
        "\n",
        "# 글자 개수 저장\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eXqUErXtciG"
      },
      "source": [
        "#### 원핫 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLTOEBNgr4QW"
      },
      "source": [
        "# 원핫 인코딩을 위한 함수\n",
        "def string_to_onehot(string):\n",
        "\n",
        "    # 입력받은 문장을 소문자로 변환\n",
        "    string = string.lower()\n",
        "\n",
        "    # 시작과 끝 토큰 생성\n",
        "    start = np.zeros(shape=n_letters, dtype=int)\n",
        "    end = np.zeros(shape=n_letters, dtype=int)\n",
        "\n",
        "    # 각 데이터에 start, end를 나타내기 위한 작업\n",
        "    ## [start] 데이터 데이터 데이터 [end]\n",
        "    ## 그래서 맨 처음 start 지점에서 iterator 돌릴 때 next()로 첫 데이터를 불러와야 함\n",
        "    start[-2] = 1\n",
        "       # BOW : Begin of Word\n",
        "    end[-1] = 1\n",
        "\n",
        "    # 입력된 문자 순회\n",
        "    for i in string:\n",
        "        # 글자의 인덱스 찾기\n",
        "        idx = char_list.index(i)\n",
        "        #print(i)\n",
        "        \n",
        "        # 각 글자를 표현하기 위한 배열 생성\n",
        "        ## 0으로 채워진 글자 길이만크의 배열 생성\n",
        "        zero = np.zeros(shape=n_letters, dtype=int)\n",
        "        ## 글자 있는 부분은 1로 채움\n",
        "        zero[idx] = 1\n",
        "        ## 시작과 끝을 이음\n",
        "        start = np.vstack([start, zero])\n",
        "    output = np.vstack([start, end])\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBEv-wyruRTc",
        "outputId": "7675fe47-4c04-4c14-dff4-651417784aaf"
      },
      "source": [
        "# 함수 실행\n",
        "print(len(string))\n",
        "print(string_to_onehot(string).shape) # start, end 더하기\n",
        "print(string_to_onehot(string))\n",
        "\n",
        "print(string_to_onehot('hi').shape) # start, end 더하기\n",
        "print(string_to_onehot('hi'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69\n",
            "(71, 33)\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]]\n",
            "(4, 33)\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batw2rjxyhEj"
      },
      "source": [
        "# 원핫인코딩 된 문자열을 복원하는 함수\n",
        "def onehot_to_word(onehot_1):\n",
        "    # 원핫인코딩 된 숫자들을 텐서로 변환\n",
        "    #onehot = torch.Tensor(onehot_1)\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    # 가장 큰 인덱스에 해당하는 글자로 리스트 생성\n",
        "    return char_list[onehot.argmax()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "EsEUPetHzM36",
        "outputId": "0dffa722-1885-44b8-8cee-9af77cf7f223"
      },
      "source": [
        "# 함수 수행\n",
        "#onehot_to_word(torch.Tensor(string_to_onehot('hi')))\n",
        "onehot_to_word((string_to_onehot('hi')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a3c879b9175c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 함수 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#onehot_to_word(torch.Tensor(string_to_onehot('hi')))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0monehot_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_to_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-cbff5bfef3ff>\u001b[0m in \u001b[0;36monehot_to_word\u001b[0;34m(onehot_1)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 원핫인코딩 된 숫자들을 텐서로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#onehot = torch.Tensor(onehot_1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0monehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 가장 큰 인덱스에 해당하는 글자로 리스트 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchar_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: descriptor 'numpy' requires a 'torch._C._TensorBase' object but received a 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-nkvWgA4Zn_"
      },
      "source": [
        "#### RNN 모델 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aalza9uK1QgR"
      },
      "source": [
        "# 모델 설계\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # 인스턴스 변수(속성, 프로퍼티)를 생성해서 값을 대입\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # RNN은 이전 층의 결과값을 가져와야 함! (입력, 출력)\n",
        "        ## 입력의 크기를 설정할 때 hidden_size에 더함\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "\n",
        "        # RNN은 활성화 함수로 tanh 함수를 사용함\n",
        "        # act_fn : activation function\n",
        "        self.act_fn = nn.Tanh()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # 현재입력과 이전출력을 붙이기\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        # 역전파를 위한 hidden 상태의 업데이트\n",
        "        hidden = self.act_fn(self.i2h(combined))\n",
        "        # 출력 만들기\n",
        "        #output = self.i2o(hidden) # 이게 아닌가? 아님!!\n",
        "        output = self.i2o(combined) # 여기서 output은 최종 출력층이 아니라 각 층의 출력부를 말함\n",
        "\n",
        "        return output, hidden # 현재 셀의 출력과, 이전 셀의 출력을 같이 가져감! RNN\n",
        "\n",
        "    # 맨 처음에는 은닉층이 없음\n",
        "    # 맨 처음의 hidden state를 만들어주는 함수\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrMUNW6mp4tc"
      },
      "source": [
        "# 모델 생성\n",
        "rnn = RNN(n_letters, n_hidden, n_letters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYmdY_O04bV-"
      },
      "source": [
        "#### 손실함수, 최적화함수(optimizer) 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il-po8Ms4O0e"
      },
      "source": [
        "# 손실함수와 최적화함수 생성\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h163OKR54q0W"
      },
      "source": [
        "#### 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCZeRhQh3OCi",
        "outputId": "824af865-8001-4495-e60a-a51e1da3de02"
      },
      "source": [
        "# 모델 훈련용 데이터 만들기\n",
        "\n",
        "# 문자열을 원핫벡터로 만들고 텐서로 변경\n",
        "# 데이터 타입 변경\n",
        "one_hot = torch.from_numpy( string_to_onehot(string) ).type(torch.FloatTensor)\n",
        "#one_hot = torch.from_numpy( string_to_onehot(string) ).as_type(torch.FloatTensor()) # 원래코드 ??\n",
        "    # type() : 함수를 변환함\n",
        "    # type_as() : 데이터를 변환함\n",
        "    # 에러가 나서 type으로 바꿔줌\n",
        "    # https://pytorch.org/docs/stable/generated/torch.Tensor.type_as.html\n",
        "\n",
        "# 확인\n",
        "print(type(one_hot))\n",
        "print(one_hot.shape)\n",
        "print(one_hot.size())\n",
        "print(type(one_hot.shape))\n",
        "print(type(one_hot.size()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([71, 33])\n",
            "torch.Size([71, 33])\n",
            "<class 'torch.Size'>\n",
            "<class 'torch.Size'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrsjJARB4_IN",
        "outputId": "25baedee-4bfa-49d4-f91d-b6683d99fba7"
      },
      "source": [
        "# 모델 훈련\n",
        "for i in range(epochs):\n",
        "    # 최적화 함수 초기화\n",
        "    optimizer.zero_grad()     # zero_grad: optimizer의 시작 지점을 0으로 초기화해줌\n",
        "                              # Sets gradients of all model parameters to zero.\n",
        "    # 첫번째 은닉층 초기화\n",
        "    hidden = rnn.init_hidden()\n",
        "    \n",
        "    # 전체 손실 저장할 변수 생성\n",
        "    total_loss = 0\n",
        "\n",
        "    # 훈련\n",
        "    for j in range(one_hot.size()[0]-1): # 70번..... 왜 -1 해주지? -2가 아니라??\n",
        "        # 입력은 앞글자\n",
        "        input_ = one_hot[j:j+1, :]\n",
        "        # 목표는 다음 글자\n",
        "        target = one_hot[j+1]\n",
        "\n",
        "        output, hidden = rnn.forward(input_, hidden)\n",
        "\n",
        "        loss = loss_func(output.view(-1), target.view(-1)) # .view??\n",
        "        total_loss += loss\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(total_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3.1007, grad_fn=<AddBackward0>)\n",
            "tensor(1.2763, grad_fn=<AddBackward0>)\n",
            "tensor(0.7439, grad_fn=<AddBackward0>)\n",
            "tensor(0.4429, grad_fn=<AddBackward0>)\n",
            "tensor(0.3041, grad_fn=<AddBackward0>)\n",
            "tensor(0.2229, grad_fn=<AddBackward0>)\n",
            "tensor(0.1578, grad_fn=<AddBackward0>)\n",
            "tensor(0.1198, grad_fn=<AddBackward0>)\n",
            "tensor(0.1130, grad_fn=<AddBackward0>)\n",
            "tensor(0.0825, grad_fn=<AddBackward0>)\n",
            "tensor(0.0686, grad_fn=<AddBackward0>)\n",
            "tensor(0.0581, grad_fn=<AddBackward0>)\n",
            "tensor(0.0697, grad_fn=<AddBackward0>)\n",
            "tensor(0.0467, grad_fn=<AddBackward0>)\n",
            "tensor(0.0417, grad_fn=<AddBackward0>)\n",
            "tensor(0.0380, grad_fn=<AddBackward0>)\n",
            "tensor(0.0352, grad_fn=<AddBackward0>)\n",
            "tensor(0.0357, grad_fn=<AddBackward0>)\n",
            "tensor(0.0330, grad_fn=<AddBackward0>)\n",
            "tensor(0.0299, grad_fn=<AddBackward0>)\n",
            "tensor(0.0277, grad_fn=<AddBackward0>)\n",
            "tensor(0.0337, grad_fn=<AddBackward0>)\n",
            "tensor(0.0268, grad_fn=<AddBackward0>)\n",
            "tensor(0.0246, grad_fn=<AddBackward0>)\n",
            "tensor(0.0230, grad_fn=<AddBackward0>)\n",
            "tensor(0.0316, grad_fn=<AddBackward0>)\n",
            "tensor(0.0226, grad_fn=<AddBackward0>)\n",
            "tensor(0.0207, grad_fn=<AddBackward0>)\n",
            "tensor(0.0194, grad_fn=<AddBackward0>)\n",
            "tensor(0.0184, grad_fn=<AddBackward0>)\n",
            "tensor(0.0316, grad_fn=<AddBackward0>)\n",
            "tensor(0.0194, grad_fn=<AddBackward0>)\n",
            "tensor(0.0176, grad_fn=<AddBackward0>)\n",
            "tensor(0.0163, grad_fn=<AddBackward0>)\n",
            "tensor(0.0154, grad_fn=<AddBackward0>)\n",
            "tensor(0.0153, grad_fn=<AddBackward0>)\n",
            "tensor(0.0191, grad_fn=<AddBackward0>)\n",
            "tensor(0.0145, grad_fn=<AddBackward0>)\n",
            "tensor(0.0138, grad_fn=<AddBackward0>)\n",
            "tensor(0.0129, grad_fn=<AddBackward0>)\n",
            "tensor(0.0132, grad_fn=<AddBackward0>)\n",
            "tensor(0.0174, grad_fn=<AddBackward0>)\n",
            "tensor(0.0139, grad_fn=<AddBackward0>)\n",
            "tensor(0.0121, grad_fn=<AddBackward0>)\n",
            "tensor(0.0113, grad_fn=<AddBackward0>)\n",
            "tensor(0.0107, grad_fn=<AddBackward0>)\n",
            "tensor(0.0103, grad_fn=<AddBackward0>)\n",
            "tensor(0.0177, grad_fn=<AddBackward0>)\n",
            "tensor(0.0122, grad_fn=<AddBackward0>)\n",
            "tensor(0.0101, grad_fn=<AddBackward0>)\n",
            "tensor(0.0093, grad_fn=<AddBackward0>)\n",
            "tensor(0.0088, grad_fn=<AddBackward0>)\n",
            "tensor(0.0084, grad_fn=<AddBackward0>)\n",
            "tensor(0.0090, grad_fn=<AddBackward0>)\n",
            "tensor(0.0108, grad_fn=<AddBackward0>)\n",
            "tensor(0.0091, grad_fn=<AddBackward0>)\n",
            "tensor(0.0080, grad_fn=<AddBackward0>)\n",
            "tensor(0.0074, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0114, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0067, grad_fn=<AddBackward0>)\n",
            "tensor(0.0064, grad_fn=<AddBackward0>)\n",
            "tensor(0.0061, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0068, grad_fn=<AddBackward0>)\n",
            "tensor(0.0064, grad_fn=<AddBackward0>)\n",
            "tensor(0.0057, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0097, grad_fn=<AddBackward0>)\n",
            "tensor(0.0068, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0048, grad_fn=<AddBackward0>)\n",
            "tensor(0.0045, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0208, grad_fn=<AddBackward0>)\n",
            "tensor(0.0066, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0195, grad_fn=<AddBackward0>)\n",
            "tensor(0.0083, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0033, grad_fn=<AddBackward0>)\n",
            "tensor(0.0031, grad_fn=<AddBackward0>)\n",
            "tensor(0.0031, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0029, grad_fn=<AddBackward0>)\n",
            "tensor(0.0027, grad_fn=<AddBackward0>)\n",
            "tensor(0.0026, grad_fn=<AddBackward0>)\n",
            "tensor(0.0025, grad_fn=<AddBackward0>)\n",
            "tensor(0.0024, grad_fn=<AddBackward0>)\n",
            "tensor(0.0023, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0187, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uvFiL_TQlHh",
        "outputId": "4ba51f93-d088-4d89-f4a3-0923690edf3c"
      },
      "source": [
        "help(np.ndarray.view) ###########################################################################\n",
        "# a.view([dtype][, type])\n",
        "# New view of array with the same data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method_descriptor:\n",
            "\n",
            "view(...)\n",
            "    a.view([dtype][, type])\n",
            "    \n",
            "    New view of array with the same data.\n",
            "    \n",
            "    .. note::\n",
            "        Passing None for ``dtype`` is different from omitting the parameter,\n",
            "        since the former invokes ``dtype(None)`` which is an alias for\n",
            "        ``dtype('float_')``.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    dtype : data-type or ndarray sub-class, optional\n",
            "        Data-type descriptor of the returned view, e.g., float32 or int16.\n",
            "        Omitting it results in the view having the same data-type as `a`.\n",
            "        This argument can also be specified as an ndarray sub-class, which\n",
            "        then specifies the type of the returned object (this is equivalent to\n",
            "        setting the ``type`` parameter).\n",
            "    type : Python type, optional\n",
            "        Type of the returned view, e.g., ndarray or matrix.  Again, omission\n",
            "        of the parameter results in type preservation.\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    ``a.view()`` is used two different ways:\n",
            "    \n",
            "    ``a.view(some_dtype)`` or ``a.view(dtype=some_dtype)`` constructs a view\n",
            "    of the array's memory with a different data-type.  This can cause a\n",
            "    reinterpretation of the bytes of memory.\n",
            "    \n",
            "    ``a.view(ndarray_subclass)`` or ``a.view(type=ndarray_subclass)`` just\n",
            "    returns an instance of `ndarray_subclass` that looks at the same array\n",
            "    (same shape, dtype, etc.)  This does not cause a reinterpretation of the\n",
            "    memory.\n",
            "    \n",
            "    For ``a.view(some_dtype)``, if ``some_dtype`` has a different number of\n",
            "    bytes per entry than the previous dtype (for example, converting a\n",
            "    regular array to a structured array), then the behavior of the view\n",
            "    cannot be predicted just from the superficial appearance of ``a`` (shown\n",
            "    by ``print(a)``). It also depends on exactly how ``a`` is stored in\n",
            "    memory. Therefore if ``a`` is C-ordered versus fortran-ordered, versus\n",
            "    defined as a slice or transpose, etc., the view may give different\n",
            "    results.\n",
            "    \n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> x = np.array([(1, 2)], dtype=[('a', np.int8), ('b', np.int8)])\n",
            "    \n",
            "    Viewing array data using a different type and dtype:\n",
            "    \n",
            "    >>> y = x.view(dtype=np.int16, type=np.matrix)\n",
            "    >>> y\n",
            "    matrix([[513]], dtype=int16)\n",
            "    >>> print(type(y))\n",
            "    <class 'numpy.matrix'>\n",
            "    \n",
            "    Creating a view on a structured array so it can be used in calculations\n",
            "    \n",
            "    >>> x = np.array([(1, 2),(3,4)], dtype=[('a', np.int8), ('b', np.int8)])\n",
            "    >>> xv = x.view(dtype=np.int8).reshape(-1,2)\n",
            "    >>> xv\n",
            "    array([[1, 2],\n",
            "           [3, 4]], dtype=int8)\n",
            "    >>> xv.mean(0)\n",
            "    array([2.,  3.])\n",
            "    \n",
            "    Making changes to the view changes the underlying array\n",
            "    \n",
            "    >>> xv[0,1] = 20\n",
            "    >>> x\n",
            "    array([(1, 20), (3,  4)], dtype=[('a', 'i1'), ('b', 'i1')])\n",
            "    \n",
            "    Using a view to convert an array to a recarray:\n",
            "    \n",
            "    >>> z = x.view(np.recarray)\n",
            "    >>> z.a\n",
            "    array([1, 3], dtype=int8)\n",
            "    \n",
            "    Views share data:\n",
            "    \n",
            "    >>> x[0] = (9, 10)\n",
            "    >>> z[0]\n",
            "    (9, 10)\n",
            "    \n",
            "    Views that change the dtype size (bytes per entry) should normally be\n",
            "    avoided on arrays defined by slices, transposes, fortran-ordering, etc.:\n",
            "    \n",
            "    >>> x = np.array([[1,2,3],[4,5,6]], dtype=np.int16)\n",
            "    >>> y = x[:, 0:2]\n",
            "    >>> y\n",
            "    array([[1, 2],\n",
            "           [4, 5]], dtype=int16)\n",
            "    >>> y.view(dtype=[('width', np.int16), ('length', np.int16)])\n",
            "    Traceback (most recent call last):\n",
            "        ...\n",
            "    ValueError: To change to a dtype of a different size, the array must be C-contiguous\n",
            "    >>> z = y.copy()\n",
            "    >>> z.view(dtype=[('width', np.int16), ('length', np.int16)])\n",
            "    array([[(1, 2)],\n",
            "           [(4, 5)]], dtype=[('width', '<i2'), ('length', '<i2')])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfdejxqd_RQS"
      },
      "source": [
        "#### ■ 슬라이싱과 copy에 대하여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADeTFGaz5mBV",
        "outputId": "283be35c-b606-4c37-e978-eacc4173b9a3"
      },
      "source": [
        "# 왜 배열 슬라이싱을 할까 ?\n",
        "print(one_hot[0:0+1, :] == one_hot[0, :])  # 값은 같지만\n",
        "print(one_hot[0:0+1, :] is one_hot[0, :])  # 다른 객체임\n",
        "## one_hot[0:0+1, :]은 copy()로서 원본 객체에 영향을 주지 않음\n",
        "## one_hot[0, :]은 참조 복사로서 변경시 원본 객체에 영향을 줌(같이 수정됨)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True]])\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seYrtMI9DElx"
      },
      "source": [
        "# ** 자연어 처리\n",
        "- NLP(Natural Language Processing)\n",
        "- Text 데이터를 분석하고 모델링하는 분야\n",
        "- 자연어 이해(Natural Language Understanding) + 자연어 생성(Natural Language Generation) 영역으로 구분하기도 함\n",
        "- 자연어 이해 : Text의 의미를 파악하는 것\n",
        "- 자연어 생성 : 주어진 의미에 대한 자연스러운 Text를 만들어내는 것\n",
        "\n",
        "## 1. 분야\n",
        "- 1) 감성 분석\n",
        "- 2) 요약\n",
        "\t- 문장의 의미를 파악하여 새로운 문장을 만들어내야 함\n",
        "- 3) 기계 번역\n",
        "- 4) 질의 응답\n",
        "\t- 현업에서는 API를 많이 이용함\n",
        "\t- 한국어 데이터셋 : korQuAD\n",
        "- 5) Pos Tagging\n",
        "\t- 형태소 분석, 품사 예측\n",
        "- 6) 챗봇\n",
        "- 7) 문장 간의 논리적인 관계에 대한 분류 모델\n",
        "\t- GAN\n",
        "\t- 창작의 영역(소설, 이미지, 음악)에서 많이 쓰임\n",
        "- 8) Image Captioning\n",
        "\t- 이미지를 설명하는 문장 붙임\n",
        "\n",
        "## 2. 자연어처리 참고 링크\n",
        "- 참고링크 : http://nlpprogress.com\n",
        "- 참고링크 : https://paperswithcode.com/area/natural-language-processing\n",
        "- 자연어 처리 리더보드\n",
        "\t- 한국 : korquad.github.io\n",
        "\t- 세계 : https://rajpurkar.github.io/SQuAD-explorer/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfpH3NJvHJz2"
      },
      "source": [
        "## 자연어 전처리\n",
        "- 숫자로 표현하는 방법(텐서, 배열)\n",
        "- 1) 문장을 숫자로 표현하기 위한 순서\n",
        "\t- Text Segmentation\n",
        "\t \t- 문장을 의미있는 단위로 나눔\n",
        "\t- Representation\n",
        "\t \t- 의미있는 부분을 숫자로 변환\n",
        "- 2) 데이터를 최소 단위로 분할하기\n",
        "\t- str 클래스의 split을 이용한 공백 단위 분할\n",
        "\t- 글자 단위로 나누기\n",
        "\t\t- 파이썬은 str이 글자의 list임\n",
        "\t\t- list(문자열) : 글자 단위로 list 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSAC9KVkHLN9"
      },
      "source": [
        "### 샘플 문장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzGEatRH62sU"
      },
      "source": [
        "# 샘플 문장 데이터\n",
        "sample1 = '다른 사람의 이야기를 듣는 건 즐겁다.'\n",
        "sample2 = '가끔 실제가 아닌 가상 인물의 이야기를 듣는 것도 좋다.'\n",
        "sample3 = '하지만 허구의 이야기는 창작자의 경험에 기반한 것으로 패턴을 알고 나면 지겹다.'\n",
        "sample4 = '요즘은 극을 보고 싶은데 이미 다 봤던 극이거나 관심 없는 극이라서 볼 게 없다.'\n",
        "sample5 = '극작이나 연출가가 본업 실력으로 떳으면 좋겠다.'\n",
        "sample6 = '소재만 자극적이고 전달하는 메세지도 또렷하지 않은 극은 재미 없고 극작의 마인드가 낡은 경우 불쾌하기까지 하다.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opFHvj5JMWU"
      },
      "source": [
        "### 데이터 최소단위로 분할하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1duxqbcuIASe",
        "outputId": "f06a4701-3636-4d5d-d31b-a8cc6bb0b243"
      },
      "source": [
        "# 단어 단위로 분할\n",
        "# str의 split 이용\n",
        "## 정규식을 기준으로 문자열 분할하여 list 생성\n",
        "## 정규식을 설정하지 않으면 공백 단위로 분할함\n",
        "print(sample1.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['다른', '사람의', '이야기를', '듣는', '건', '즐겁다.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyXGNwCsIKvS",
        "outputId": "16ca50e1-6693-4a77-d921-45f1c0c888a0"
      },
      "source": [
        "# 글자 단위로 분할\n",
        "# 파이썬의 str은 문자열의 list임을 이용\n",
        "print(list(sample2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['가', '끔', ' ', '실', '제', '가', ' ', '아', '닌', ' ', '가', '상', ' ', '인', '물', '의', ' ', '이', '야', '기', '를', ' ', '듣', '는', ' ', '것', '도', ' ', '좋', '다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO5pfGbJJPQc"
      },
      "source": [
        "### 분할한 데이터를 수치화하기\n",
        "- 중복된 단어는 1번만 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibFeM7ynIz3t",
        "outputId": "967739a9-109b-4dc3-8e54-9f224a125502"
      },
      "source": [
        "# 공백을 기준으로 분할한 단어를 수치화하여 딕셔너리에 저장\n",
        "\n",
        "# 데이터를 저장할 딕셔너리 생성\n",
        "token2idx = {}\n",
        "index = 0\n",
        "\n",
        "# 문장들에 대하여\n",
        "for sentence in [sample1, sample2, sample3, sample4, sample5, sample6]:\n",
        "    # 각 문장을 단어 단위로 분할\n",
        "    tokens = sentence.split()\n",
        "    # 분할한 문장을 단어 단위로 순회함\n",
        "    for token in tokens:\n",
        "        # dict에 존재하지 않으면 추가함 - key는 중복되지 않으므로 ! 중복 단어는 1번만 수행됨\n",
        "        # 인덱스 값을 1씩 증가시켜서 저장\n",
        "        if token2idx.get(token) == None:\n",
        "            token2idx[token] = index\n",
        "            index += 1\n",
        "\n",
        "print(token2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'다른': 0, '사람의': 1, '이야기를': 2, '듣는': 3, '건': 4, '즐겁다.': 5, '가끔': 6, '실제가': 7, '아닌': 8, '가상': 9, '인물의': 10, '것도': 11, '좋다.': 12, '하지만': 13, '허구의': 14, '이야기는': 15, '창작자의': 16, '경험에': 17, '기반한': 18, '것으로': 19, '패턴을': 20, '알고': 21, '나면': 22, '지겹다.': 23, '요즘은': 24, '극을': 25, '보고': 26, '싶은데': 27, '이미': 28, '다': 29, '봤던': 30, '극이거나': 31, '관심': 32, '없는': 33, '극이라서': 34, '볼': 35, '게': 36, '없다.': 37, '극작이나': 38, '연출가가': 39, '본업': 40, '실력으로': 41, '떳으면': 42, '좋겠다.': 43, '소재만': 44, '자극적이고': 45, '전달하는': 46, '메세지도': 47, '또렷하지': 48, '않은': 49, '극은': 50, '재미': 51, '없고': 52, '극작의': 53, '마인드가': 54, '낡은': 55, '경우': 56, '불쾌하기까지': 57, '하다.': 58}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5bkNgSzI9ea"
      },
      "source": [
        "# 문장 안의 token을 key로 이용하여 token2idx 딕셔너리에서 숫자(index)를 꺼내옴\n",
        "def indexed_sentence(sentence):\n",
        "    return [token2idx[token] for token in sentence]\n",
        "    # token은 sentence 안에 있는 경우만 수행하기 때문에, 새로운 단어가 오면 오류 남"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R234_VJ7Nw6_",
        "outputId": "9eaf1b3b-9825-4484-bef8-1386b5bc062a"
      },
      "source": [
        "# 함수 수행\n",
        "result = indexed_sentence(sample1.split())\n",
        "print(result)\n",
        "\n",
        "result = indexed_sentence(sample2.split())\n",
        "print(result)\n",
        "print()\n",
        "\n",
        "print(\"모든 샘플에 대해 인덱스 꺼내보기\")\n",
        "for i, sample in enumerate([sample1, sample2, sample3, sample4, sample5, sample6]):\n",
        "    result = indexed_sentence(sample.split())\n",
        "    print(\"sample\", i+1, \":\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5]\n",
            "[6, 7, 8, 9, 10, 2, 3, 11, 12]\n",
            "\n",
            "모든 샘플에 대해 인덱스 꺼내보기\n",
            "sample 1 : [0, 1, 2, 3, 4, 5]\n",
            "sample 2 : [6, 7, 8, 9, 10, 2, 3, 11, 12]\n",
            "sample 3 : [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
            "sample 4 : [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]\n",
            "sample 5 : [38, 39, 40, 41, 42, 43]\n",
            "sample 6 : [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am4_7eZzOIfA"
      },
      "source": [
        "# 문제점\n",
        "# 위 딕셔너리에 없는 문자가 오면 작동하지 않음\n",
        "#result = indexed_sentence(\"이번에도 호프가 올까?\".split())\n",
        "#print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvJ9oekXPdrp"
      },
      "source": [
        "### OOV를 위한 token 만들기\n",
        "- 딕셔너리에 없는 token이 오면 수행하지 않음\n",
        "- 해결방법 : OOV를 위한 token을 별도로 만들기\n",
        "\t- OOV : Out of Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLmOXp89P4Vl"
      },
      "source": [
        "# OOV를 위한 별도의 토큰 추가\n",
        "\n",
        "## 아는 단어가 나오면 값을 1 추가\n",
        "token2idx = {t:i+1 for t, i in token2idx.items()}\n",
        "    # token2idx.items() : key, val 튜플\n",
        "\n",
        "## 모르는 단어가 나올 때 불러올 키, 값 생성\n",
        "token2idx['<unknown>'] = 0\n",
        "\n",
        "## 함수 수정\n",
        "def indexed_sentence(sentence):\n",
        "    return [token2idx.get(token, token2idx['<unknown>']) for token in sentence]\n",
        "    # 딕셔너리.get(입력한 키, 해당 키가 없을 때 리턴할 디폴트 키)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0XGCBjDR51Z",
        "outputId": "0b8e0465-7a6d-47bd-d438-250476311db7"
      },
      "source": [
        "help(dict.get)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method_descriptor:\n",
            "\n",
            "get(self, key, default=None, /)\n",
            "    Return the value for key if key is in the dictionary, else default.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fILAP6oVO_6g",
        "outputId": "e7cd4592-c29d-4526-8d30-8b518c53a774"
      },
      "source": [
        "# 이젠 새로운 단어가 오면 0값을 리턴함\n",
        "result = indexed_sentence(\"이번에도 호프가 올까?\".split())\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4MZkMlCWzak"
      },
      "source": [
        "## Corpus\n",
        "- Corpus : token을 만들기 위해서 모아 놓은 문장의 모음\n",
        "- 크롤링하여 생성\n",
        "- 이미 만들어진 corpus 사용\n",
        "\t- 영어 corpus : https://www.english-corpora.org\n",
        "\t- 구글의 corpus는 740GB가 넘음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3BnQJADW5BD"
      },
      "source": [
        "\n",
        "## BPE 알고리즘 구현\n",
        "- Byte Pair Encoding\n",
        "- 1) 한국어의 단위\n",
        "\t- 음운 < 음절 < 형태소 < 단어 < 어절 < 문장\n",
        "\t\t- 음운 : 소리의 단위\n",
        "\t\t- 음절 : 글자\n",
        "\t\t- 형태소 : 의미를 가진 최소 단위\n",
        "\t\t- 단어 : 최소의 자립 형식. 일반적으로 공백 기준으로 분할. 조사는 한 단어임을 유의.\n",
        "\t\t- 어절V : 문장을 이루는 마디. 띄어쓰기의 단위.\n",
        "\t- 영어는 단어와 어절이 동일하여 띄어쓰기로 분할\n",
        "\t- 한국어는 조사를 뗀 단어가 필요함\n",
        "- 2) '글자' 단위로 사전을 생성하는 경우\n",
        "\t- 더 적은 사전으로 여러 문장 처리 가능함\n",
        "\t- OOV 문제가 해결됨\n",
        "\t- 하지만 글자는 그 자체로 의미를 갖지 않는 경우가 대부분임\n",
        "- 3) n_gram Tokenization\n",
        "\t- 글자, 글자 결합에 초점을 둠\n",
        "\t- uni_gram : 하나의 글자만 이용하는 경우\n",
        "\t- b i_gram : 2개의 글자를 이용하는 경우\n",
        "\t- 문제점\n",
        "\t \t- 이 방법은 단어 사전이 너무 커짐\n",
        "\t\t- 한 번만 등장하고 다시는 등장하지 않는 단어도 사전에 등록해야 하기 때문임\n",
        "- 4) BPE 이용\n",
        "\t- n_gram 중에서 사용이 될 것 같은 단어만 모은 것\n",
        "\t- 반복적으로 나오는 데이터의 패턴을 치환하는 방식을 사용함\n",
        "\t- 데이터를 효율적으로 저장하기 위한 압축 알고리즘\n",
        "\n",
        "- 예시\n",
        "\t- abbcabcab\n",
        "\t- step 1) 가장 많이 등장하는 2개짜리 패턴을 찾아서 X로 치환\n",
        "\t\t- ab가 제일 많이 등장\n",
        "\t\t- XbcXcX\n",
        "\t- step 2) 반복 :  패턴이 없어질 때까지/혹은 일정 횟수 이상\n",
        "\t\t- Y = cX\n",
        "\t\t- XbYY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUETCQ9ARwIT"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "# 단어의 등장 횟수를 dict에 저장하는 함수  ???????????\n",
        "# vocab은 딕셔너리\n",
        "def get_stats(vocab):\n",
        "    # 정수 딕셔너리 생성\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):    # <\\w>를 빼고 세려고 -1 함\n",
        "            pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2PLFJueXl6s"
      },
      "source": [
        "- help(collections.defaultdict(int))\n",
        "```\n",
        "class defaultdict(builtins.dict)\n",
        " |  defaultdict(default_factory[, ...]) --> dict with default factory\n",
        " |  \n",
        " |  The default factory is called without arguments to produce\n",
        " |  a new value when a key is not present, in __getitem__ only.\n",
        " |  A defaultdict compares equal to a dict with the same items.\n",
        " |  All remaining arguments are treated the same as if they were\n",
        " |  passed to the dict constructor, including keyword arguments.\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWJMeHqRXYwV"
      },
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    #p = re.compile(r'(?<!\\S)' + bigram + r'(?<!\\S)')\n",
        "    # <가 메타문자에서 뭐길래..?\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "# re : 정규표현식 메타문자\n",
        "# https://developer.mozilla.org/ko/docs/Web/JavaScript/Guide/Regular_Expressions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHk4l4Dlo2TT",
        "outputId": "16e6b9d2-226b-42e2-8fa9-483e8bf6fd8a"
      },
      "source": [
        "# 테스트할 단어 생성\n",
        "vocab = ({'l o w </w>':5,        # 키 : l o w </w>, 값 : 5\n",
        "          'l o w e r </w>':2,\n",
        "          'n e w e s t </w>':6,\n",
        "          'w i d e s t </w>':3})\n",
        "\n",
        "# 수행횟수 설정\n",
        "num_merges = 10\n",
        "for i in range(num_merges):\n",
        "    # 연속된 2글자의 등장 횟수 셈\n",
        "    pairs = get_stats(vocab)\n",
        "    # 가장 많이 등장한 페어를 찾음\n",
        "    best = max(pairs, key = pairs.get)\n",
        "    # 가잔 많이 등장한 페어의 공백 제거\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f'Step {i+1}')\n",
        "    print(best)\n",
        "    print(vocab)\n",
        "    print('\\n')\n",
        "\n",
        "#get_stats(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "('e', 's')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
            "\n",
            "\n",
            "Step 2\n",
            "('es', 't')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
            "\n",
            "\n",
            "Step 3\n",
            "('est', '</w>')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 4\n",
            "('l', 'o')\n",
            "{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 5\n",
            "('lo', 'w')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 6\n",
            "('n', 'e')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 7\n",
            "('ne', 'w')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 8\n",
            "('new', 'est</w>')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 9\n",
            "('low', '</w>')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 10\n",
            "('w', 'i')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4WhrzEKFpRY"
      },
      "source": [
        "## 원핫인코딩 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBmvof0arTYs"
      },
      "source": [
        "# 단어사전 만드는 데이터\n",
        "s1 = \"이게 뭐지 이게 뭘까 자고 일어나니 세상이 바뀐 것처럼\"\n",
        "s2 = \"뮤지컬 보고싶다. 본 적 없고 연출 좋고 노래 좋은 뮤지컬\"\n",
        "s3 = \"무신경한 연출과 어디서 들어본 것 같은 대사는 이제 지겨워\"\n",
        "s4 = \"배우가 좋으면 그 순간은 좋겠지. 곱씹을수록 별로라 그렇지\"\n",
        "s5 = \"대극장 뮤지컬이 보고싶은 걸까?\"\n",
        "s6 = \"마틸다랑 라이언킹은 연출도 내용도 좋았다.\""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpYdM9wGGMKW",
        "outputId": "70233675-bfd0-4b07-8539-0687653e8335"
      },
      "source": [
        "# 딕셔너리 생성\n",
        "token2idx = {}\n",
        "index = 0\n",
        "for sentence in [s1, s2, s3, s4, s5, s6]:\n",
        "    # 각 문장을 공백 기준으로 나누기\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        # 각 띄어쓰기 단어\n",
        "        if token2idx.get(token) == None:\n",
        "            token2idx[token] = index\n",
        "            index += 1\n",
        "print(token2idx)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'이게': 0, '뭐지': 1, '뭘까': 2, '자고': 3, '일어나니': 4, '세상이': 5, '바뀐': 6, '것처럼': 7, '뮤지컬': 8, '보고싶다.': 9, '본': 10, '적': 11, '없고': 12, '연출': 13, '좋고': 14, '노래': 15, '좋은': 16, '무신경한': 17, '연출과': 18, '어디서': 19, '들어본': 20, '것': 21, '같은': 22, '대사는': 23, '이제': 24, '지겨워': 25, '배우가': 26, '좋으면': 27, '그': 28, '순간은': 29, '좋겠지.': 30, '곱씹을수록': 31, '별로라': 32, '그렇지': 33, '대극장': 34, '뮤지컬이': 35, '보고싶은': 36, '걸까?': 37, '마틸다랑': 38, '라이언킹은': 39, '연출도': 40, '내용도': 41, '좋았다.': 42}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfgd-PH1Gi3m",
        "outputId": "a69945ce-af8d-4c0d-ade4-d43e3227134d"
      },
      "source": [
        "# 샘플데이터에 등장한 단어의 개수\n",
        "V = len(token2idx)\n",
        "\n",
        "# 단어가 등장한 idx가 아니면 0을 채워넣음.\n",
        "# 배열, ㅎㅎ\n",
        "token2vec = [(   [0 if i != idx else 1 for i in range(V)]   , idx, token)    for token, idx in token2idx.items()]\n",
        "\n",
        "for x in token2vec:\n",
        "    print('\\t'.join([str(y) for y in x]))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t0\t이게\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t1\t뭐지\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t2\t뭘까\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t3\t자고\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t4\t일어나니\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t5\t세상이\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t6\t바뀐\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t7\t것처럼\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t8\t뮤지컬\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t9\t보고싶다.\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t10\t본\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t11\t적\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t12\t없고\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t13\t연출\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t14\t좋고\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t15\t노래\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t16\t좋은\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t17\t무신경한\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t18\t연출과\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t19\t어디서\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t20\t들어본\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t21\t것\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t22\t같은\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t23\t대사는\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t24\t이제\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t25\t지겨워\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t26\t배우가\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t27\t좋으면\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t28\t그\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t29\t순간은\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t30\t좋겠지.\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t31\t곱씹을수록\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t32\t별로라\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t33\t그렇지\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\t34\t대극장\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\t35\t뮤지컬이\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\t36\t보고싶은\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\t37\t걸까?\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\t38\t마틸다랑\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\t39\t라이언킹은\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\t40\t연출도\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\t41\t내용도\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\t42\t좋았다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icGYEIRWHaI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2950ff5d-6200-4ad0-b8fc-20be89909508"
      },
      "source": [
        "# 각 샘플 문장을 원핫 인코딩 하기\n",
        "import numpy as np\n",
        "for sentence in [s1, s2, s3]:\n",
        "    onehot_s = []\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        if token2idx.get(token) != None:\n",
        "            vector = np.zeros((1, V))\n",
        "            vector[:, token2idx[token]] = 1\n",
        "            onehot_s.append(vector)\n",
        "        else:\n",
        "            print(\"unknown\")\n",
        "    print(f\"{sentence}:\")\n",
        "    print((np.concatenate(onehot_s, axis=0)).shape)\n",
        "    print(np.concatenate(onehot_s, axis=0))    # ndarray    # shape: (n, 43)\n",
        "    print()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이게 뭐지 이게 뭘까 자고 일어나니 세상이 바뀐 것처럼:\n",
            "(9, 43)\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "뮤지컬 보고싶다. 본 적 없고 연출 좋고 노래 좋은 뮤지컬:\n",
            "(10, 43)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "무신경한 연출과 어디서 들어본 것 같은 대사는 이제 지겨워:\n",
            "(9, 43)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvugHNqhWkXq"
      },
      "source": [
        "## Pre-Trained Word Embedding 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzgqO8nLWsMg"
      },
      "source": [
        "### google의 Sentencepiece 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSxycSXMWLAJ",
        "outputId": "7ffcd5c2-b88e-42b7-a6c4-d510a513b2d0"
      },
      "source": [
        "# 라이브러리 설치\n",
        "pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzffIxfEJb4d",
        "outputId": "5df2cdf2-7cd6-4c0e-a0fd-428682b12c62"
      },
      "source": [
        "# 모델 다운로드\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
        "# !wget -P /경로/ 다운받을파일링크\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-18 08:22:57--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "\rbotchan.txt           0%[                    ]       0  --.-KB/s               \rbotchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-08-18 08:22:57 (13.0 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLDKWCYdWIcC",
        "outputId": "b701af6b-3252-4d5a-e9cf-112c896fd1a7"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# 모델에 데이터 설정\n",
        "spm.SentencePieceTrainer.train('--input=/content/botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "                                                    # 모델 경로    # 모델 이름 설정?\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmyrQab5YMp3",
        "outputId": "2e78736b-d7f6-4008-afbd-88f6ea9afc88"
      },
      "source": [
        "# 단어 단위 토큰화\n",
        "## 토큰 리턴\n",
        "print(sp.encode_as_pieces('New York'))\n",
        "## 아이디 리턴\n",
        "print(sp.encode_as_ids('New York'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁New', '▁Y', 'or', 'k']\n",
            "[1437, 867, 105, 94]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw5IeTLzZiGr",
        "outputId": "790a1987-a7a7-4844-8fcd-c60a3cfd1b99"
      },
      "source": [
        "# 복원\n",
        "## 토큰을 이용하여 복원\n",
        "print(sp.decode_pieces(['▁New', '▁Y', 'or', 'k']))\n",
        "## 아이디를 이용하여 복원\n",
        "print(sp.decode_ids([1437, 867, 105, 94]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New York\n",
            "New York\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxn6YQw6aacg"
      },
      "source": [
        "### BERT 이용\n",
        "- Transformers 설치\n",
        "- BERT의 버전\n",
        "    - 기본 버전 : 'bert-base-uncased'\n",
        "    - 다국어 버전 : 'bert-base-multilingual-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XiGOaxMaDLw",
        "outputId": "9affc7fe-8c30-4b0b-e8a3-46e2a7cc33b3"
      },
      "source": [
        "# BERT 사용을 위한 Transformers 설치\n",
        "! pip install transformers"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyfsEDCJanKV",
        "outputId": "803a8b0c-9017-4685-f45c-aa8e6ce94e6e"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# 기본 버전 가져오기\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(len(tokenizer.vocab))    # 약 30522개의 단어가 있음\n",
        "\n",
        "# 토큰화 테스트\n",
        "print(tokenizer.tokenize('defalult version only supports English'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30522\n",
            "['def', '##al', '##ult', 'version', 'only', 'supports', 'english']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhVDHMPca9yt",
        "outputId": "79e3435a-b187-4b6f-b214-4965a8e1c75c"
      },
      "source": [
        "# 다국어 버전 가져오기\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "print(len(tokenizer.vocab))    # 약 30522개의 단어가 있음\n",
        "\n",
        "# 토큰화 테스트\n",
        "print(tokenizer.tokenize('다국어 version은 한국어 지원함. 근데 영어로 섞여도 잘 될까?'))\n",
        "## 고작 '지'를 몰라서 ㅈ + ㅣ로 나눈다고..?"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105879\n",
            "['다', '##국', '##어', 'version', '##은', '한국', '##어', 'ᄌ', '##ᅵ', '##원', '##함', '.', '그', '##ᆫ', '##데', '영어', '##로', 'ᄉ', '##ᅥ', '##ᆩ', '##여', '##도', '잘', '될', '##ᄁ', '##ᅡ', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-hc6k9nbfbU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}