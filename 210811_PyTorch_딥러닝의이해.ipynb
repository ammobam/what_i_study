{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210811 PyTorch_딥러닝의이해.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1e-Mi2Xh0VFtYFcZvrcWU9-dSE0MMcwlq",
      "authorship_tag": "ABX9TyP55EfdmbYcUBgajHeGs/wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammobam/Study_DeepLearing/blob/main/210811_PyTorch_%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%98%EC%9D%B4%ED%95%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAZIHTAnp7yy",
        "outputId": "fc12b1f3-64c5-478a-f8fa-5b0928373ec5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5zynmxQ4KKI"
      },
      "source": [
        "# ** Torch 소개\n",
        "- facebook이 만든 Python을 위한 오픈소스 머신러닝 라이브러링\n",
        "- GPU 사용이 가능하기 때문에 속도가 빠름\n",
        "- 최근에는 PyTorch 사용자가 늘어나고 있음\n",
        "- PyTorch의 장점 : Microsoft의 프레임워크가 모델전환이 가능함\n",
        "\n",
        "\n",
        "- 다른 딥러닝 패키지 : TensorFlow\n",
        "    - 먼저 배포되어 현재까지 사용자가 많으나 비직관적인 구조, 학습 난이도가 높음\n",
        "    - TensorFlow는 비직관적인 구조를 개선하기 위해 2.0에서 Keras를 포집함\n",
        "\n",
        "\n",
        "\n",
        "1) Pytorch 설치\n",
        "\n",
        "```\n",
        "\tpip install torch torchvision\n",
        "\tconda install pytorch torchvision -c pytorch\n",
        "```\n",
        "\n",
        "2) GPU 사용을 위한 설정\n",
        "- CUDA 설치\n",
        "\t- https://developer.nvidia.com/cuda-toolkit-archive\n",
        "\n",
        "- cuDNN 설치\n",
        "\t- https://developer.nvidia.com/cudnn\n",
        "\n",
        "- 윈도우에 GPU 확인하는 방법\n",
        "\t- 작업관리자 > 성능 > GPU 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KRtWmgpxb5z"
      },
      "source": [
        "## Torch 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk1kcZt5uvd9",
        "outputId": "e2836fe1-e35d-4705-9833-7a4c00d2c198"
      },
      "source": [
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9631, 0.6426, 0.8523],\n",
            "        [0.8878, 0.2459, 0.8171],\n",
            "        [0.1911, 0.1547, 0.5760],\n",
            "        [0.5163, 0.3672, 0.5157],\n",
            "        [0.8142, 0.5548, 0.0964]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVCRLO03yAnz"
      },
      "source": [
        "## Tensor\n",
        "- Tensor\n",
        "- 데이터를 표현하는 단위\n",
        "- Scalar, Vector, Matmul, Tensor등이 있음\n",
        "\n",
        "1) Scalar\n",
        "- 하나의 값\n",
        "\n",
        "2) Vector\n",
        "- 원래 의미는 데이터 모임을 Vector라고 함\n",
        "- 프로그래밍 언어에서는 Collection이라고 표현함\n",
        "\n",
        "- 1차원으로 모인 데이터의 집합\n",
        "- Vector끼리 연산을 하면 동일한 위치의 데이터끼리 연산을 수행함\n",
        "- torch에서 numpy의 ndarray 연산 대부분을 제공함\n",
        "\n",
        "3) Matmul\n",
        "- 행과 열 구조\n",
        "- 벡터의 모임\n",
        "- 내적을 구하는 메소드로 matmul을 지원함\n",
        "\n",
        "4) Tensor\n",
        "- Matrix의 배열"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rodMuVLx-NP",
        "outputId": "b6da2fff-12e7-46bc-9070-d21be16b60b0"
      },
      "source": [
        "# 하나의 값을 스칼라 데이터라고 함\n",
        "scalar_data = torch.tensor([1.0])\n",
        "print(\"스칼라 데이터:\", scalar_data)\n",
        "\n",
        "# Vector : 1차원 데이터의 모임\n",
        "vector1 = torch.tensor([1.0, 2.0])\n",
        "vector2 = torch.tensor([4.0, 5.0])\n",
        "\n",
        "# 동일한 인덱스의 데이터끼리 연산\n",
        "# 2개 벡터 크기가 다르면 연산이 불가함\n",
        "print(\"벡터의 덧셈:\", vector1 + vector2)\n",
        "\n",
        "# torch는 numpy의 거의 모든 연산을 지원함\n",
        "# 행렬연산. 행, 열 단위로 계산함\n",
        "# 하나의 벡터를 1개의 열로 간주하여 연산 수행함\n",
        "print(\"벡터의 내적:\", torch.dot(vector1, vector2))    # 1x2 dot 2x1 --> 1x1\n",
        "\n",
        "# Matrix (행렬)\n",
        "matrix1 = torch.tensor([[1, 2], [3, 4]])    # 2x2\n",
        "matrix2 = torch.tensor([[5, 6], [7, 8]])    # 2x2\n",
        "print(\"행렬연산 지원:\\n\", torch.matmul(matrix1, matrix2))    # 2x2\n",
        "\n",
        "# Tensor : Matrix의 배열\n",
        "tensor1 = torch.tensor([[[1, 2], [3, 4]], [[2, 2], [2, 2]]])    # 2x2 행렬 2개의 배열\n",
        "print(tensor1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "스칼라 데이터: tensor([1.])\n",
            "벡터의 덧셈: tensor([5., 7.])\n",
            "벡터의 내적: tensor(14.)\n",
            "행렬연산 지원:\n",
            " tensor([[19, 22],\n",
            "        [43, 50]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[2, 2],\n",
            "         [2, 2]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsix3EvSmCCt"
      },
      "source": [
        "# ** MLP 설계를 통한 딥러닝의 이해\n",
        "- MNIST 데이터셋 이용\n",
        "- Pytorch는 클래스를 수정하여 모델을 생성, 훈련, 평가함\n",
        "---\n",
        "## 과정\n",
        "- 필요한 모듈 불러오기\n",
        "- 장비 확인\n",
        "- 데이터 다운로드 및 분할\n",
        "- 데이터 확인\n",
        "- MLP 모델 설계\n",
        "- Optimizer, Object Function 설정\n",
        "- 학습진행 중 모델 성능을 확인하는 함수 정의\n",
        "- 학습진행 중 검증 데이터에 대한 모델 성능을 확인하는 함수 정의\n",
        "- 학습진행 중 학습/테스트 데이터의 Loss 및 Accuracy 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NtSkQm3muhs"
      },
      "source": [
        "## 필요한 모듈 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Dt4GzKmyxM"
      },
      "source": [
        "# 필요한 모듈 불러오기\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pytorch 기본모듈\n",
        "import torch\n",
        "\n",
        "# 딥러닝 모델을 만들 때 필요한 함수 모듈\n",
        "import torch.nn as nn\n",
        "# 자주 사용하는 함수 모듈\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 데이터 관련 모듈\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-fWxJa2kIU"
      },
      "source": [
        "## 장비 확인 - GPU 사용가능 여부 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE5Hwq6czofc",
        "outputId": "8a9dc44c-12a0-441f-925b-6cecb72580f5"
      },
      "source": [
        "# Colab GPU를 켜고 확인해보면 True가 나옴\n",
        "print(\"GPU 사용가능 여부:\", torch.cuda.is_available())\n",
        "\n",
        "# 조건문으로 확인 가능\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('CPU')\n",
        "\n",
        "print('torch version:', torch.__version__)\n",
        "print('DEVICE:', DEVICE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 사용가능 여부: True\n",
            "torch version: 1.9.0+cu102\n",
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af2sTwG6nRsE"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HeO5lQOsbc4"
      },
      "source": [
        "### 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq22B1qTn2wc"
      },
      "source": [
        "# torch 에서 지원하는 MNIST 손글씨 데이터 가져오기\n",
        "# 데이터를 다운로드 받아서 경로에 저장하고\n",
        "# transform을 설정하면 텐서로 변경하고 0~1 사이로 정규화함\n",
        "train_datasets = datasets.MNIST(root='/content/drive/MyDrive/data/MNIST', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_datasets = datasets.MNIST(root='/content/drive/MyDrive/data/MNIST', train=False, download=True, transform=transforms.ToTensor())\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP6_EMtVo_Fc"
      },
      "source": [
        "# 한 번에 훈련하는 크기\n",
        "BATCH_SIZE = 32\n",
        "# 훈련 횟수\n",
        "EPOCHS = 10"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHu4M_xeqL8S"
      },
      "source": [
        "# pytorch 딥러닝에서 사용할 형태로 데이터 변환\n",
        "# 여기서 데이터 섞는 옵션 줄 수 있음\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_datasets, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_datasets, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU9JycMksYJx"
      },
      "source": [
        "### 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Bh9gGFrPCy",
        "outputId": "9033711e-0415-4c41-d6e5-84cb64eac0a0"
      },
      "source": [
        "# 데이터 확인\n",
        "print(dir(train_loader))\n",
        "print(dir(test_loader))\n",
        "\n",
        "# 순환 가능한 데이터\n",
        "for (X_train, y_train) in train_loader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break\n",
        "\n",
        "# 훈련 데이터는 28*28 짜리 흑백 이미지이고 32개 씩 묶여 있음\n",
        "# 훈련 레이블은 32개 짜리 스칼라 데이터가 묶여 있음"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n",
            "['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n",
            "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
            "y_train: torch.Size([32]) type: torch.LongTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "JeNGxMF-sTB5",
        "outputId": "7b522241-8bba-4cbc-d296-f57149105f2c"
      },
      "source": [
        "# 데이터 꺼내보기\n",
        "pltsize = 1\n",
        "plt.figure(figsize=(10*pltsize, pltsize))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap='gray_r')\n",
        "    plt.title('Class:' + str(y_train[i].item()))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJEalRT3r7vR"
      },
      "source": [
        "## MLP 모델 설계\n",
        "- pytorch는 클래스를 상속받아서 모델을 설계함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5k_cd4br-wn"
      },
      "source": [
        "# 모델 설계\n",
        "class Net(nn.Module):\n",
        "    # 생성자 - 초기화 메소드\n",
        "    ## 인스턴스를 만들기 위해서 호출하는 함수\n",
        "    def __init__(self):\n",
        "        # 상위 클래스의 생성자를 호출\n",
        "        ## Java, C++은 상위 클래스 생성자 호출하지 않음. 파이썬은 함.\n",
        "        super(Net, self).__init__()\n",
        "        # 인스턴스 변수 초기화 - 층을 쌓음\n",
        "        self.fc1 = nn.Linear(28*28, 512)   # 입력층\n",
        "        self.fc2 = nn.Linear(512, 256)     # 은닉층\n",
        "        self.fc3 = nn.Linear(256, 10)      # 출력층\n",
        "\n",
        "    # Forward Propagation을 위한 함수\n",
        "    def forward(self, x):\n",
        "        # 입력 데이터를 만들기 위해서 28*28의 1차원 데이터로 펼침\n",
        "        x = x.view(-1, 28*28)\n",
        "        \n",
        "        # 입력층을 활성화해서 다음으로 넘김\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        \n",
        "        # 은닉층을 활성화해서 다음으로 넘김\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        \n",
        "        # 출력층을 활성화함\n",
        "        # 활성화함수 softmax, log_softmax의 차이?\n",
        "        # 로그값을 이용하면 역전파할 때 그래프의 기울기가 부드럽게 변하여 그라디언트 소실 문제를 예방함\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX_NE0dLzc79",
        "outputId": "c5c8a92c-e8aa-4aae-d93a-183d055a362a"
      },
      "source": [
        "# 모델 생성\n",
        "model = Net().to(DEVICE)\n",
        "## Net() : 클래스 불러와서 모델 객체 생성\n",
        "## .to(DEVICE) : 장치 할당\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91zPOiwgsB0g"
      },
      "source": [
        "## Optimizer, Object Function 설정\n",
        "- Gradient Descent Method(경사하강법)\n",
        "    - loss를 최소화하는 방법\n",
        "- Optimizer\n",
        "    - 경사하강법을 구현하는 함수\n",
        "    - loss를 최소화하는 함수\n",
        "- Optimizer 종류\n",
        "    - SGD : 데이터를 미분하여 loss를 최소화해나가는 방법\n",
        "\t    - Stochastic Gradient Descent\n",
        "    - SGD 외 여러 함수 이용 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf0CijuRzZQF"
      },
      "source": [
        "# Optimizer 정의\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "## lr : 학습률\n",
        "## momentum : 관성, 일종의 가속도\n",
        "\n",
        "# Loss Function 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "## criterion : 표준\n",
        "## CrossEntropyLoss\n",
        "    ## 다중분류를 위한 함수\n",
        "    ## nn.LogSoftmax와 nn.NLLLoss의 연산의 조합"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo57RDp8O_nL"
      },
      "source": [
        "* nn.CrossEntropyLoss에 대하여\n",
        "* http://www.gisdeveloper.co.kr/?p=8668"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49FpFY-IsJgo"
      },
      "source": [
        "## 학습진행 중 모델 성능을 확인하는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY9zVR6_sKfq"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    # 모델 훈련 시작\n",
        "    model.train()\n",
        "    # 훈련 데이터의 배치번호, 이미지, 레이블을 순회함\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        # image, label을 DEVICE에 할당\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        # optimizer 초기화\n",
        "        optimizer.zero_grad()\n",
        "        # image로 예측\n",
        "        output = model(image)\n",
        "        # 손실 계산\n",
        "        loss = criterion(output, label)\n",
        "        # 손실 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 파라미터 값(weight)을 업데이트\n",
        "        ## weight를 업데이트 하고 사용한 optimizer는 None으로 초기화 하는 것!\n",
        "        ## loss가 적을 때의 weight만 업데이트하고 다시 for문\n",
        "        optimizer.step()\n",
        "\n",
        "        # 출력\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch:{}[{}/{} {:.0f}%]\\tTrain Loss:{:.6f}'.\n",
        "                  format(epoch, batch_idx*len(image), len(train_loader.dataset), 100*batch_idx/len(train_loader), loss.item()))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvr4qFPp2k4z"
      },
      "source": [
        "### help(optimizer)\n",
        "\n",
        "```\n",
        " |  zero_grad(self, set_to_none: bool = False)\n",
        " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
        " |      \n",
        " |      Args:\n",
        " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
        " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
        " |              However, it changes certain behaviors. For example:\n",
        " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
        " |              a None attribute or a Tensor full of 0s will behave differently.\n",
        " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
        " |              are guaranteed to be None for params that did not receive a gradient.\n",
        " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
        " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
        " |              the step altogether).\n",
        "```\n",
        "```\n",
        " |  step(self, closure=None)\n",
        " |      Performs a single optimization step.\n",
        " |      \n",
        " |      Args:\n",
        " |          closure (callable, optional): A closure that reevaluates the model\n",
        " |              and returns the loss.\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OnBrd88sK3X"
      },
      "source": [
        "## 학습진행 중 검증 데이터에 대한 모델 성능을 확인하는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pazqgqTPsL0T"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    # 모델 검증 시작\n",
        "    model.eval()\n",
        "    # 누적 손실을 계산하기 위한 변수\n",
        "    test_loss = 0\n",
        "    # 정확히 분류된 데이터의 개수를 세는 변수\n",
        "    correct_cnt = 0\n",
        "\n",
        "    # 훈련 데이터의 배치번호, 이미지, 레이블을 순회함\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            # image, label을 DEVICE에 할당\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "\n",
        "            # image로 예측\n",
        "            output = model(image)\n",
        "                ## output : 어떤 클래스로 분류할기 확률을 리턴한 텐서\n",
        "\n",
        "            # 누적 손실 계산\n",
        "            test_loss += criterion(output, label)\n",
        "\n",
        "            # 정답의 개수를 계산\n",
        "            prediction = output.max(1, keepdim=True)[1]\n",
        "                ## ouput 텐서에서 가장 큰 값(확률)의 클래스를 리턴함\n",
        "                ## keepdim : 차원도 같이 리턴하는 옵션\n",
        "            correct_cnt += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "                ## 결과적으로, 예측값과 레이블값이 동일하면 1 더해주는 코드임\n",
        "                ## .view_as : label은 텐서가 아니라서 prediction과 같은 타입으로 바꿔줌  #??정확한 기능 찾아보기\n",
        "                ## .eq: prediction과, label의 값이 같은지 비교\n",
        "                ## .sum().item() : True일 때의 개수를 리턴함                \n",
        "        \n",
        "        # 손실의 평균 구하기\n",
        "        test_loss /= len(test_loader.dataset)    # /= : 나눠서 다시 왼쪽 변수에 할당\n",
        "        # 정확도 구하기\n",
        "        test_accuracy = 100 * correct_cnt / len(test_loader.dataset)\n",
        "        return test_loss, test_accuracy"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCDPgkcsMKP"
      },
      "source": [
        "## 학습진행 중 학습/테스트 데이터의 Loss 및 Accuracy 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPSmG0038Lx2"
      },
      "source": [
        "### 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_orgAcQ8LR_",
        "outputId": "9e721f9f-3906-4fe5-cc78-8d2a32b3dbe8"
      },
      "source": [
        "# 정의한 함수 불러와서 모델 훈련하기\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # 훈련\n",
        "    train(model , train_loader, optimizer, log_interval=200)\n",
        "    # 검증해서 loss, accuracy 받아오기\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    # 출력\n",
        "    print('\\n[EPOCH:{}]\\ \\tTest Loss : {:.4f} \\tTest Accuracy:{:.2f}%\\n'.format(epoch, test_loss, test_accuracy))\n",
        "\n",
        "# [EPOCH:10]\\ \tTest Loss : 0.0105 \tTest Accuracy:90.03%\n",
        "# 훈련할수록 Loss는 감소하고 Accuracy는 증가하는 것을 확인할 수 있음"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch:1[0/60000 0%]\tTrain Loss:2.313113\n",
            "Train Epoch:1[6400/60000 11%]\tTrain Loss:2.280902\n",
            "Train Epoch:1[12800/60000 21%]\tTrain Loss:2.335389\n",
            "Train Epoch:1[19200/60000 32%]\tTrain Loss:2.323482\n",
            "Train Epoch:1[25600/60000 43%]\tTrain Loss:2.324605\n",
            "Train Epoch:1[32000/60000 53%]\tTrain Loss:2.309533\n",
            "Train Epoch:1[38400/60000 64%]\tTrain Loss:2.302251\n",
            "Train Epoch:1[44800/60000 75%]\tTrain Loss:2.273365\n",
            "Train Epoch:1[51200/60000 85%]\tTrain Loss:2.232270\n",
            "Train Epoch:1[57600/60000 96%]\tTrain Loss:2.216110\n",
            "\n",
            "[EPOCH:1]\\ \tTest Loss : 0.0694 \tTest Accuracy:30.45%\n",
            "\n",
            "Train Epoch:2[0/60000 0%]\tTrain Loss:2.181501\n",
            "Train Epoch:2[6400/60000 11%]\tTrain Loss:2.175222\n",
            "Train Epoch:2[12800/60000 21%]\tTrain Loss:2.260787\n",
            "Train Epoch:2[19200/60000 32%]\tTrain Loss:2.014546\n",
            "Train Epoch:2[25600/60000 43%]\tTrain Loss:1.962686\n",
            "Train Epoch:2[32000/60000 53%]\tTrain Loss:1.845089\n",
            "Train Epoch:2[38400/60000 64%]\tTrain Loss:1.629709\n",
            "Train Epoch:2[44800/60000 75%]\tTrain Loss:1.527965\n",
            "Train Epoch:2[51200/60000 85%]\tTrain Loss:1.449652\n",
            "Train Epoch:2[57600/60000 96%]\tTrain Loss:1.605603\n",
            "\n",
            "[EPOCH:2]\\ \tTest Loss : 0.0388 \tTest Accuracy:67.32%\n",
            "\n",
            "Train Epoch:3[0/60000 0%]\tTrain Loss:1.415351\n",
            "Train Epoch:3[6400/60000 11%]\tTrain Loss:1.068305\n",
            "Train Epoch:3[12800/60000 21%]\tTrain Loss:0.982917\n",
            "Train Epoch:3[19200/60000 32%]\tTrain Loss:1.110085\n",
            "Train Epoch:3[25600/60000 43%]\tTrain Loss:0.996785\n",
            "Train Epoch:3[32000/60000 53%]\tTrain Loss:0.922818\n",
            "Train Epoch:3[38400/60000 64%]\tTrain Loss:0.822744\n",
            "Train Epoch:3[44800/60000 75%]\tTrain Loss:0.726629\n",
            "Train Epoch:3[51200/60000 85%]\tTrain Loss:0.754814\n",
            "Train Epoch:3[57600/60000 96%]\tTrain Loss:0.553737\n",
            "\n",
            "[EPOCH:3]\\ \tTest Loss : 0.0235 \tTest Accuracy:77.96%\n",
            "\n",
            "Train Epoch:4[0/60000 0%]\tTrain Loss:0.606905\n",
            "Train Epoch:4[6400/60000 11%]\tTrain Loss:0.732905\n",
            "Train Epoch:4[12800/60000 21%]\tTrain Loss:0.599112\n",
            "Train Epoch:4[19200/60000 32%]\tTrain Loss:0.564721\n",
            "Train Epoch:4[25600/60000 43%]\tTrain Loss:0.451938\n",
            "Train Epoch:4[32000/60000 53%]\tTrain Loss:0.666092\n",
            "Train Epoch:4[38400/60000 64%]\tTrain Loss:0.560268\n",
            "Train Epoch:4[44800/60000 75%]\tTrain Loss:0.952939\n",
            "Train Epoch:4[51200/60000 85%]\tTrain Loss:0.416113\n",
            "Train Epoch:4[57600/60000 96%]\tTrain Loss:0.523258\n",
            "\n",
            "[EPOCH:4]\\ \tTest Loss : 0.0175 \tTest Accuracy:84.05%\n",
            "\n",
            "Train Epoch:5[0/60000 0%]\tTrain Loss:0.822870\n",
            "Train Epoch:5[6400/60000 11%]\tTrain Loss:0.363536\n",
            "Train Epoch:5[12800/60000 21%]\tTrain Loss:0.740127\n",
            "Train Epoch:5[19200/60000 32%]\tTrain Loss:0.464631\n",
            "Train Epoch:5[25600/60000 43%]\tTrain Loss:0.425929\n",
            "Train Epoch:5[32000/60000 53%]\tTrain Loss:0.705039\n",
            "Train Epoch:5[38400/60000 64%]\tTrain Loss:0.309926\n",
            "Train Epoch:5[44800/60000 75%]\tTrain Loss:0.467464\n",
            "Train Epoch:5[51200/60000 85%]\tTrain Loss:0.286222\n",
            "Train Epoch:5[57600/60000 96%]\tTrain Loss:0.393939\n",
            "\n",
            "[EPOCH:5]\\ \tTest Loss : 0.0145 \tTest Accuracy:86.65%\n",
            "\n",
            "Train Epoch:6[0/60000 0%]\tTrain Loss:0.793626\n",
            "Train Epoch:6[6400/60000 11%]\tTrain Loss:0.276175\n",
            "Train Epoch:6[12800/60000 21%]\tTrain Loss:0.425371\n",
            "Train Epoch:6[19200/60000 32%]\tTrain Loss:0.229160\n",
            "Train Epoch:6[25600/60000 43%]\tTrain Loss:0.611671\n",
            "Train Epoch:6[32000/60000 53%]\tTrain Loss:0.371792\n",
            "Train Epoch:6[38400/60000 64%]\tTrain Loss:0.540897\n",
            "Train Epoch:6[44800/60000 75%]\tTrain Loss:0.519419\n",
            "Train Epoch:6[51200/60000 85%]\tTrain Loss:0.452418\n",
            "Train Epoch:6[57600/60000 96%]\tTrain Loss:0.595177\n",
            "\n",
            "[EPOCH:6]\\ \tTest Loss : 0.0131 \tTest Accuracy:87.58%\n",
            "\n",
            "Train Epoch:7[0/60000 0%]\tTrain Loss:0.665470\n",
            "Train Epoch:7[6400/60000 11%]\tTrain Loss:0.281973\n",
            "Train Epoch:7[12800/60000 21%]\tTrain Loss:0.174823\n",
            "Train Epoch:7[19200/60000 32%]\tTrain Loss:0.370720\n",
            "Train Epoch:7[25600/60000 43%]\tTrain Loss:0.251484\n",
            "Train Epoch:7[32000/60000 53%]\tTrain Loss:0.232178\n",
            "Train Epoch:7[38400/60000 64%]\tTrain Loss:0.655965\n",
            "Train Epoch:7[44800/60000 75%]\tTrain Loss:0.426047\n",
            "Train Epoch:7[51200/60000 85%]\tTrain Loss:0.482162\n",
            "Train Epoch:7[57600/60000 96%]\tTrain Loss:0.366603\n",
            "\n",
            "[EPOCH:7]\\ \tTest Loss : 0.0120 \tTest Accuracy:88.90%\n",
            "\n",
            "Train Epoch:8[0/60000 0%]\tTrain Loss:0.289682\n",
            "Train Epoch:8[6400/60000 11%]\tTrain Loss:0.382784\n",
            "Train Epoch:8[12800/60000 21%]\tTrain Loss:0.276648\n",
            "Train Epoch:8[19200/60000 32%]\tTrain Loss:0.434797\n",
            "Train Epoch:8[25600/60000 43%]\tTrain Loss:0.128426\n",
            "Train Epoch:8[32000/60000 53%]\tTrain Loss:0.462122\n",
            "Train Epoch:8[38400/60000 64%]\tTrain Loss:0.386565\n",
            "Train Epoch:8[44800/60000 75%]\tTrain Loss:0.456458\n",
            "Train Epoch:8[51200/60000 85%]\tTrain Loss:0.257932\n",
            "Train Epoch:8[57600/60000 96%]\tTrain Loss:0.327230\n",
            "\n",
            "[EPOCH:8]\\ \tTest Loss : 0.0114 \tTest Accuracy:89.55%\n",
            "\n",
            "Train Epoch:9[0/60000 0%]\tTrain Loss:0.210846\n",
            "Train Epoch:9[6400/60000 11%]\tTrain Loss:0.394500\n",
            "Train Epoch:9[12800/60000 21%]\tTrain Loss:0.525804\n",
            "Train Epoch:9[19200/60000 32%]\tTrain Loss:0.935130\n",
            "Train Epoch:9[25600/60000 43%]\tTrain Loss:0.554022\n",
            "Train Epoch:9[32000/60000 53%]\tTrain Loss:0.602577\n",
            "Train Epoch:9[38400/60000 64%]\tTrain Loss:0.323722\n",
            "Train Epoch:9[44800/60000 75%]\tTrain Loss:0.432627\n",
            "Train Epoch:9[51200/60000 85%]\tTrain Loss:0.220912\n",
            "Train Epoch:9[57600/60000 96%]\tTrain Loss:0.162612\n",
            "\n",
            "[EPOCH:9]\\ \tTest Loss : 0.0109 \tTest Accuracy:90.00%\n",
            "\n",
            "Train Epoch:10[0/60000 0%]\tTrain Loss:0.596142\n",
            "Train Epoch:10[6400/60000 11%]\tTrain Loss:0.158334\n",
            "Train Epoch:10[12800/60000 21%]\tTrain Loss:0.264012\n",
            "Train Epoch:10[19200/60000 32%]\tTrain Loss:0.192708\n",
            "Train Epoch:10[25600/60000 43%]\tTrain Loss:0.240109\n",
            "Train Epoch:10[32000/60000 53%]\tTrain Loss:0.384707\n",
            "Train Epoch:10[38400/60000 64%]\tTrain Loss:0.775324\n",
            "Train Epoch:10[44800/60000 75%]\tTrain Loss:0.513929\n",
            "Train Epoch:10[51200/60000 85%]\tTrain Loss:0.164877\n",
            "Train Epoch:10[57600/60000 96%]\tTrain Loss:0.478534\n",
            "\n",
            "[EPOCH:10]\\ \tTest Loss : 0.0105 \tTest Accuracy:90.03%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMrvhBnR24i"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz1kgST-R6TR"
      },
      "source": [
        "# 딥러닝 모델을 개선하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro5-tAfERkum"
      },
      "source": [
        "## Dropout을 적용한 신경망\n",
        "- 과적합과 경사소실에 대한 문제를 해결하기 위한 알고리즘\n",
        "\n",
        "(1) 효과\n",
        "- 신경망 학습 중 Layer의 노드를 랜덤하게 Drop함으로서 Generalization 효과를 냄\n",
        "\n",
        "(2) 구동원리\n",
        "- 실제 뉴런을 삭제하는 게 아님. 행렬에 0을 대입하여 연산 수행하는 것.\n",
        "-- 대다수의 머신러닝 알고리즘은 input data, weight, hidden layer 모두 행렬 연산 수행함\n",
        "\n",
        "- 누락할 뉴런 비율을 설정함\n",
        "\n",
        "(3) 활용\n",
        "- 신경망 설계시 많이 이용하는 기술임\n",
        "- 훈련 데이터에 대한 과적합을 방지하여 결과적으로 모델 정확도가 좋아짐\n",
        "- 주의 : 충분히 많은 epoch 수를 설정하여 훈련하고 비교해야 Dropout 효과를 알 수 있음\n",
        "\n",
        "(4) Pytorch에서 구현 방법\n",
        "- self.dropout_prob를 설정하고\n",
        "- 출력층을 제외한 각 층의 끝에 설정함\n",
        "- 출력층에 dropout을 설정하지 않는 이유 : 이득이 없고 성능만 떨어짐\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI97BoCDRopf"
      },
      "source": [
        "# 모델 수정 - dropout 추가\n",
        "\n",
        "class Net(nn.Module):\n",
        "    # 생성자 - 초기화 메소드\n",
        "    def __init__(self):\n",
        "        # 상위 클래스의 생성자를 호출\n",
        "        super(Net, self).__init__()\n",
        "        # 인스턴스 변수 초기화 - 층을 쌓음\n",
        "        self.fc1 = nn.Linear(28*28, 512)   # 입력층\n",
        "        self.fc2 = nn.Linear(512, 256)     # 은닉층\n",
        "        self.fc3 = nn.Linear(256, 10)      # 출력층\n",
        "\n",
        "        # dropout 설정 추가\n",
        "        self.dropout_prob = 0.5\n",
        "\n",
        "    # Forward Propagation을 위한 함수\n",
        "    def forward(self, x):\n",
        "        # 입력 데이터를 만들기 위해서 28*28의 1차원 데이터로 펼침\n",
        "        x = x.view(-1, 28*28)\n",
        "        \n",
        "        # 입력층을 활성화해서 다음으로 넘김\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        # dropout 추가\n",
        "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
        "\n",
        "        \n",
        "        # 은닉층을 활성화해서 다음으로 넘김\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        # dropout 추가\n",
        "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
        "        \n",
        "        # 출력층을 활성화함\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        # 출력층에는 dropout을 설정하지 않음 ! 성능을 떨어뜨릴 뿐, 이득이 없음.\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSNse022T5qx"
      },
      "source": [
        "# 모델 생성\n",
        "model = Net().to(DEVICE)\n",
        "\n",
        "# Optimizer 정의\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "# Loss Function 정의\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqDSIf6PUCDP",
        "outputId": "26eceac3-da50-41fb-fcfa-8e213f64ab04"
      },
      "source": [
        "# 모델 훈련\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # 훈련\n",
        "    train(model , train_loader, optimizer, log_interval=200)\n",
        "    # 검증해서 loss, accuracy 받아오기\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    # 출력\n",
        "    print('\\n[EPOCH:{}]\\ \\tTest Loss : {:.4f} \\tTest Accuracy:{:.2f}%\\n'.format(epoch, test_loss, test_accuracy))\n",
        "\n",
        "# Dropout 효과 확인\n",
        "# 전 : [EPOCH:10]\\ \tTest Loss : 0.0105 \tTest Accuracy:90.03%\n",
        "# 후 : [EPOCH:10]\\ \tTest Loss : 0.0103 \tTest Accuracy:90.29%\n",
        "# epoch 수가 적어서 Dropout의 효과를 알기 어려움\n",
        "# Dropout으로 뉴런의 일부 값을 잃기 때문에 적은 epoch에서는 오히려 모델 성능이 나빠질 가능성도 있음"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch:1[0/60000 0%]\tTrain Loss:2.313703\n",
            "Train Epoch:1[6400/60000 11%]\tTrain Loss:2.318047\n",
            "Train Epoch:1[12800/60000 21%]\tTrain Loss:2.282327\n",
            "Train Epoch:1[19200/60000 32%]\tTrain Loss:2.313968\n",
            "Train Epoch:1[25600/60000 43%]\tTrain Loss:2.295457\n",
            "Train Epoch:1[32000/60000 53%]\tTrain Loss:2.243975\n",
            "Train Epoch:1[38400/60000 64%]\tTrain Loss:2.262578\n",
            "Train Epoch:1[44800/60000 75%]\tTrain Loss:2.269325\n",
            "Train Epoch:1[51200/60000 85%]\tTrain Loss:2.328547\n",
            "Train Epoch:1[57600/60000 96%]\tTrain Loss:2.251366\n",
            "\n",
            "[EPOCH:1]\\ \tTest Loss : 0.0695 \tTest Accuracy:33.98%\n",
            "\n",
            "Train Epoch:2[0/60000 0%]\tTrain Loss:2.244982\n",
            "Train Epoch:2[6400/60000 11%]\tTrain Loss:2.182992\n",
            "Train Epoch:2[12800/60000 21%]\tTrain Loss:2.180601\n",
            "Train Epoch:2[19200/60000 32%]\tTrain Loss:2.047811\n",
            "Train Epoch:2[25600/60000 43%]\tTrain Loss:2.092475\n",
            "Train Epoch:2[32000/60000 53%]\tTrain Loss:1.841512\n",
            "Train Epoch:2[38400/60000 64%]\tTrain Loss:1.626981\n",
            "Train Epoch:2[44800/60000 75%]\tTrain Loss:1.439450\n",
            "Train Epoch:2[51200/60000 85%]\tTrain Loss:1.500099\n",
            "Train Epoch:2[57600/60000 96%]\tTrain Loss:1.264195\n",
            "\n",
            "[EPOCH:2]\\ \tTest Loss : 0.0386 \tTest Accuracy:66.25%\n",
            "\n",
            "Train Epoch:3[0/60000 0%]\tTrain Loss:1.101766\n",
            "Train Epoch:3[6400/60000 11%]\tTrain Loss:1.130491\n",
            "Train Epoch:3[12800/60000 21%]\tTrain Loss:1.078328\n",
            "Train Epoch:3[19200/60000 32%]\tTrain Loss:0.933993\n",
            "Train Epoch:3[25600/60000 43%]\tTrain Loss:1.056363\n",
            "Train Epoch:3[32000/60000 53%]\tTrain Loss:0.699210\n",
            "Train Epoch:3[38400/60000 64%]\tTrain Loss:0.810201\n",
            "Train Epoch:3[44800/60000 75%]\tTrain Loss:0.944378\n",
            "Train Epoch:3[51200/60000 85%]\tTrain Loss:0.591111\n",
            "Train Epoch:3[57600/60000 96%]\tTrain Loss:0.598370\n",
            "\n",
            "[EPOCH:3]\\ \tTest Loss : 0.0237 \tTest Accuracy:77.70%\n",
            "\n",
            "Train Epoch:4[0/60000 0%]\tTrain Loss:0.779183\n",
            "Train Epoch:4[6400/60000 11%]\tTrain Loss:0.707513\n",
            "Train Epoch:4[12800/60000 21%]\tTrain Loss:0.778977\n",
            "Train Epoch:4[19200/60000 32%]\tTrain Loss:0.625036\n",
            "Train Epoch:4[25600/60000 43%]\tTrain Loss:0.473246\n",
            "Train Epoch:4[32000/60000 53%]\tTrain Loss:0.919799\n",
            "Train Epoch:4[38400/60000 64%]\tTrain Loss:0.534047\n",
            "Train Epoch:4[44800/60000 75%]\tTrain Loss:0.432051\n",
            "Train Epoch:4[51200/60000 85%]\tTrain Loss:0.791829\n",
            "Train Epoch:4[57600/60000 96%]\tTrain Loss:0.567530\n",
            "\n",
            "[EPOCH:4]\\ \tTest Loss : 0.0175 \tTest Accuracy:83.60%\n",
            "\n",
            "Train Epoch:5[0/60000 0%]\tTrain Loss:0.321320\n",
            "Train Epoch:5[6400/60000 11%]\tTrain Loss:0.519475\n",
            "Train Epoch:5[12800/60000 21%]\tTrain Loss:0.380291\n",
            "Train Epoch:5[19200/60000 32%]\tTrain Loss:0.456002\n",
            "Train Epoch:5[25600/60000 43%]\tTrain Loss:0.387021\n",
            "Train Epoch:5[32000/60000 53%]\tTrain Loss:0.461455\n",
            "Train Epoch:5[38400/60000 64%]\tTrain Loss:0.567795\n",
            "Train Epoch:5[44800/60000 75%]\tTrain Loss:0.482719\n",
            "Train Epoch:5[51200/60000 85%]\tTrain Loss:0.364323\n",
            "Train Epoch:5[57600/60000 96%]\tTrain Loss:0.514712\n",
            "\n",
            "[EPOCH:5]\\ \tTest Loss : 0.0142 \tTest Accuracy:87.05%\n",
            "\n",
            "Train Epoch:6[0/60000 0%]\tTrain Loss:0.456992\n",
            "Train Epoch:6[6400/60000 11%]\tTrain Loss:0.326146\n",
            "Train Epoch:6[12800/60000 21%]\tTrain Loss:0.372137\n",
            "Train Epoch:6[19200/60000 32%]\tTrain Loss:0.439710\n",
            "Train Epoch:6[25600/60000 43%]\tTrain Loss:0.320669\n",
            "Train Epoch:6[32000/60000 53%]\tTrain Loss:0.575854\n",
            "Train Epoch:6[38400/60000 64%]\tTrain Loss:0.708860\n",
            "Train Epoch:6[44800/60000 75%]\tTrain Loss:0.549097\n",
            "Train Epoch:6[51200/60000 85%]\tTrain Loss:0.212419\n",
            "Train Epoch:6[57600/60000 96%]\tTrain Loss:0.429494\n",
            "\n",
            "[EPOCH:6]\\ \tTest Loss : 0.0127 \tTest Accuracy:88.18%\n",
            "\n",
            "Train Epoch:7[0/60000 0%]\tTrain Loss:0.228579\n",
            "Train Epoch:7[6400/60000 11%]\tTrain Loss:0.197803\n",
            "Train Epoch:7[12800/60000 21%]\tTrain Loss:0.467515\n",
            "Train Epoch:7[19200/60000 32%]\tTrain Loss:0.370276\n",
            "Train Epoch:7[25600/60000 43%]\tTrain Loss:0.159772\n",
            "Train Epoch:7[32000/60000 53%]\tTrain Loss:1.048970\n",
            "Train Epoch:7[38400/60000 64%]\tTrain Loss:0.384499\n",
            "Train Epoch:7[44800/60000 75%]\tTrain Loss:0.334874\n",
            "Train Epoch:7[51200/60000 85%]\tTrain Loss:0.297880\n",
            "Train Epoch:7[57600/60000 96%]\tTrain Loss:0.148771\n",
            "\n",
            "[EPOCH:7]\\ \tTest Loss : 0.0117 \tTest Accuracy:89.02%\n",
            "\n",
            "Train Epoch:8[0/60000 0%]\tTrain Loss:0.462868\n",
            "Train Epoch:8[6400/60000 11%]\tTrain Loss:0.385705\n",
            "Train Epoch:8[12800/60000 21%]\tTrain Loss:0.308631\n",
            "Train Epoch:8[19200/60000 32%]\tTrain Loss:0.639933\n",
            "Train Epoch:8[25600/60000 43%]\tTrain Loss:0.187301\n",
            "Train Epoch:8[32000/60000 53%]\tTrain Loss:0.537251\n",
            "Train Epoch:8[38400/60000 64%]\tTrain Loss:0.216566\n",
            "Train Epoch:8[44800/60000 75%]\tTrain Loss:0.274595\n",
            "Train Epoch:8[51200/60000 85%]\tTrain Loss:0.308743\n",
            "Train Epoch:8[57600/60000 96%]\tTrain Loss:0.259904\n",
            "\n",
            "[EPOCH:8]\\ \tTest Loss : 0.0112 \tTest Accuracy:89.59%\n",
            "\n",
            "Train Epoch:9[0/60000 0%]\tTrain Loss:0.608772\n",
            "Train Epoch:9[6400/60000 11%]\tTrain Loss:0.322217\n",
            "Train Epoch:9[12800/60000 21%]\tTrain Loss:0.424774\n",
            "Train Epoch:9[19200/60000 32%]\tTrain Loss:0.216360\n",
            "Train Epoch:9[25600/60000 43%]\tTrain Loss:0.408571\n",
            "Train Epoch:9[32000/60000 53%]\tTrain Loss:0.313608\n",
            "Train Epoch:9[38400/60000 64%]\tTrain Loss:0.108271\n",
            "Train Epoch:9[44800/60000 75%]\tTrain Loss:0.384065\n",
            "Train Epoch:9[51200/60000 85%]\tTrain Loss:0.341973\n",
            "Train Epoch:9[57600/60000 96%]\tTrain Loss:0.336165\n",
            "\n",
            "[EPOCH:9]\\ \tTest Loss : 0.0108 \tTest Accuracy:90.08%\n",
            "\n",
            "Train Epoch:10[0/60000 0%]\tTrain Loss:0.309768\n",
            "Train Epoch:10[6400/60000 11%]\tTrain Loss:0.334849\n",
            "Train Epoch:10[12800/60000 21%]\tTrain Loss:0.354752\n",
            "Train Epoch:10[19200/60000 32%]\tTrain Loss:0.405705\n",
            "Train Epoch:10[25600/60000 43%]\tTrain Loss:0.319697\n",
            "Train Epoch:10[32000/60000 53%]\tTrain Loss:0.456018\n",
            "Train Epoch:10[38400/60000 64%]\tTrain Loss:0.329057\n",
            "Train Epoch:10[44800/60000 75%]\tTrain Loss:0.318440\n",
            "Train Epoch:10[51200/60000 85%]\tTrain Loss:0.330937\n",
            "Train Epoch:10[57600/60000 96%]\tTrain Loss:0.163030\n",
            "\n",
            "[EPOCH:10]\\ \tTest Loss : 0.0103 \tTest Accuracy:90.29%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC0qkdidX2Y-"
      },
      "source": [
        "## 활성화 함수가 ReLU인 신경망"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx84pownX-ju"
      },
      "source": [
        "# 모델 수정 - dropout 추가\n",
        "\n",
        "class Net(nn.Module):\n",
        "    # 생성자 - 초기화 메소드\n",
        "    def __init__(self):\n",
        "        # 상위 클래스의 생성자를 호출\n",
        "        super(Net, self).__init__()\n",
        "        # 인스턴스 변수 초기화 - 층을 쌓음\n",
        "        self.fc1 = nn.Linear(28*28, 512)   # 입력층\n",
        "        self.fc2 = nn.Linear(512, 256)     # 은닉층\n",
        "        self.fc3 = nn.Linear(256, 10)      # 출력층\n",
        "\n",
        "        # dropout 설정 추가\n",
        "        self.dropout_prob = 0.5\n",
        "\n",
        "    # Forward Propagation을 위한 함수\n",
        "    def forward(self, x):\n",
        "        # 입력 데이터를 만들기 위해서 28*28의 1차원 데이터로 펼침\n",
        "        x = x.view(-1, 28*28)\n",
        "        \n",
        "        # 입력층을 활성화해서 다음으로 넘김\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # dropout 추가\n",
        "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
        "\n",
        "        \n",
        "        # 은닉층을 활성화해서 다음으로 넘김\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        # dropout 추가\n",
        "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
        "        \n",
        "        # 출력층을 활성화함\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        # 출력층에는 dropout을 설정하지 않음 ! 성능을 떨어뜨릴 뿐, 이득이 없음.\n",
        "\n",
        "        return x"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkC_jWp9X-ju"
      },
      "source": [
        "# 모델 생성\n",
        "model = Net().to(DEVICE)\n",
        "\n",
        "# Optimizer 정의\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "# Loss Function 정의\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1FJvEAUX-ju",
        "outputId": "f67bb940-6a44-493c-a737-b4f3624102a5"
      },
      "source": [
        "# 모델 훈련\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # 훈련\n",
        "    train(model , train_loader, optimizer, log_interval=200)\n",
        "    # 검증해서 loss, accuracy 받아오기\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    # 출력\n",
        "    print('\\n[EPOCH:{}]\\ \\tTest Loss : {:.4f} \\tTest Accuracy:{:.2f}%\\n'.format(epoch, test_loss, test_accuracy))\n",
        "\n",
        "# 활성화함수 relu 효과 확인\n",
        "# 전 : [EPOCH:10]\\ \tTest Loss : 0.0103 \tTest Accuracy:90.29%\n",
        "# 후 : [EPOCH:10]\\ \tTest Loss : 0.0027 \tTest Accuracy:97.40%\n",
        "# Loss는 감소하고 Accuracy는 증가함\n",
        "# epoch 수가 적어서 ReLU의 효과를 알기 어려움\n",
        "# 활성화함수들을 비교해보면 epoch 수가 충분히 커질 때 성능이 차이남"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch:1[0/60000 0%]\tTrain Loss:2.292025\n",
            "Train Epoch:1[6400/60000 11%]\tTrain Loss:2.068696\n",
            "Train Epoch:1[12800/60000 21%]\tTrain Loss:1.315995\n",
            "Train Epoch:1[19200/60000 32%]\tTrain Loss:0.666023\n",
            "Train Epoch:1[25600/60000 43%]\tTrain Loss:0.627972\n",
            "Train Epoch:1[32000/60000 53%]\tTrain Loss:0.720902\n",
            "Train Epoch:1[38400/60000 64%]\tTrain Loss:0.572086\n",
            "Train Epoch:1[44800/60000 75%]\tTrain Loss:0.495520\n",
            "Train Epoch:1[51200/60000 85%]\tTrain Loss:0.237104\n",
            "Train Epoch:1[57600/60000 96%]\tTrain Loss:0.445136\n",
            "\n",
            "[EPOCH:1]\\ \tTest Loss : 0.0100 \tTest Accuracy:90.84%\n",
            "\n",
            "Train Epoch:2[0/60000 0%]\tTrain Loss:0.438757\n",
            "Train Epoch:2[6400/60000 11%]\tTrain Loss:0.453742\n",
            "Train Epoch:2[12800/60000 21%]\tTrain Loss:0.291302\n",
            "Train Epoch:2[19200/60000 32%]\tTrain Loss:0.353523\n",
            "Train Epoch:2[25600/60000 43%]\tTrain Loss:0.342037\n",
            "Train Epoch:2[32000/60000 53%]\tTrain Loss:0.338289\n",
            "Train Epoch:2[38400/60000 64%]\tTrain Loss:0.231173\n",
            "Train Epoch:2[44800/60000 75%]\tTrain Loss:0.564083\n",
            "Train Epoch:2[51200/60000 85%]\tTrain Loss:0.219222\n",
            "Train Epoch:2[57600/60000 96%]\tTrain Loss:0.292490\n",
            "\n",
            "[EPOCH:2]\\ \tTest Loss : 0.0071 \tTest Accuracy:93.31%\n",
            "\n",
            "Train Epoch:3[0/60000 0%]\tTrain Loss:0.296917\n",
            "Train Epoch:3[6400/60000 11%]\tTrain Loss:0.312748\n",
            "Train Epoch:3[12800/60000 21%]\tTrain Loss:0.222546\n",
            "Train Epoch:3[19200/60000 32%]\tTrain Loss:0.558233\n",
            "Train Epoch:3[25600/60000 43%]\tTrain Loss:0.085960\n",
            "Train Epoch:3[32000/60000 53%]\tTrain Loss:0.203975\n",
            "Train Epoch:3[38400/60000 64%]\tTrain Loss:0.332533\n",
            "Train Epoch:3[44800/60000 75%]\tTrain Loss:0.194152\n",
            "Train Epoch:3[51200/60000 85%]\tTrain Loss:0.114302\n",
            "Train Epoch:3[57600/60000 96%]\tTrain Loss:0.183914\n",
            "\n",
            "[EPOCH:3]\\ \tTest Loss : 0.0054 \tTest Accuracy:94.87%\n",
            "\n",
            "Train Epoch:4[0/60000 0%]\tTrain Loss:0.733554\n",
            "Train Epoch:4[6400/60000 11%]\tTrain Loss:0.195264\n",
            "Train Epoch:4[12800/60000 21%]\tTrain Loss:0.324975\n",
            "Train Epoch:4[19200/60000 32%]\tTrain Loss:0.192171\n",
            "Train Epoch:4[25600/60000 43%]\tTrain Loss:0.185550\n",
            "Train Epoch:4[32000/60000 53%]\tTrain Loss:0.242182\n",
            "Train Epoch:4[38400/60000 64%]\tTrain Loss:0.071811\n",
            "Train Epoch:4[44800/60000 75%]\tTrain Loss:0.256948\n",
            "Train Epoch:4[51200/60000 85%]\tTrain Loss:0.265695\n",
            "Train Epoch:4[57600/60000 96%]\tTrain Loss:0.136075\n",
            "\n",
            "[EPOCH:4]\\ \tTest Loss : 0.0045 \tTest Accuracy:95.67%\n",
            "\n",
            "Train Epoch:5[0/60000 0%]\tTrain Loss:0.095860\n",
            "Train Epoch:5[6400/60000 11%]\tTrain Loss:0.114942\n",
            "Train Epoch:5[12800/60000 21%]\tTrain Loss:0.217800\n",
            "Train Epoch:5[19200/60000 32%]\tTrain Loss:0.142062\n",
            "Train Epoch:5[25600/60000 43%]\tTrain Loss:0.100483\n",
            "Train Epoch:5[32000/60000 53%]\tTrain Loss:0.309891\n",
            "Train Epoch:5[38400/60000 64%]\tTrain Loss:0.278420\n",
            "Train Epoch:5[44800/60000 75%]\tTrain Loss:0.133568\n",
            "Train Epoch:5[51200/60000 85%]\tTrain Loss:0.395500\n",
            "Train Epoch:5[57600/60000 96%]\tTrain Loss:0.494767\n",
            "\n",
            "[EPOCH:5]\\ \tTest Loss : 0.0039 \tTest Accuracy:96.09%\n",
            "\n",
            "Train Epoch:6[0/60000 0%]\tTrain Loss:0.314959\n",
            "Train Epoch:6[6400/60000 11%]\tTrain Loss:0.181422\n",
            "Train Epoch:6[12800/60000 21%]\tTrain Loss:0.139457\n",
            "Train Epoch:6[19200/60000 32%]\tTrain Loss:0.337658\n",
            "Train Epoch:6[25600/60000 43%]\tTrain Loss:0.235195\n",
            "Train Epoch:6[32000/60000 53%]\tTrain Loss:0.238660\n",
            "Train Epoch:6[38400/60000 64%]\tTrain Loss:0.070450\n",
            "Train Epoch:6[44800/60000 75%]\tTrain Loss:0.612417\n",
            "Train Epoch:6[51200/60000 85%]\tTrain Loss:0.101089\n",
            "Train Epoch:6[57600/60000 96%]\tTrain Loss:0.119283\n",
            "\n",
            "[EPOCH:6]\\ \tTest Loss : 0.0035 \tTest Accuracy:96.40%\n",
            "\n",
            "Train Epoch:7[0/60000 0%]\tTrain Loss:0.122808\n",
            "Train Epoch:7[6400/60000 11%]\tTrain Loss:0.081148\n",
            "Train Epoch:7[12800/60000 21%]\tTrain Loss:0.232924\n",
            "Train Epoch:7[19200/60000 32%]\tTrain Loss:0.055684\n",
            "Train Epoch:7[25600/60000 43%]\tTrain Loss:0.259237\n",
            "Train Epoch:7[32000/60000 53%]\tTrain Loss:0.124061\n",
            "Train Epoch:7[38400/60000 64%]\tTrain Loss:0.108402\n",
            "Train Epoch:7[44800/60000 75%]\tTrain Loss:0.042450\n",
            "Train Epoch:7[51200/60000 85%]\tTrain Loss:0.033612\n",
            "Train Epoch:7[57600/60000 96%]\tTrain Loss:0.276586\n",
            "\n",
            "[EPOCH:7]\\ \tTest Loss : 0.0031 \tTest Accuracy:96.82%\n",
            "\n",
            "Train Epoch:8[0/60000 0%]\tTrain Loss:0.170257\n",
            "Train Epoch:8[6400/60000 11%]\tTrain Loss:0.054169\n",
            "Train Epoch:8[12800/60000 21%]\tTrain Loss:0.099256\n",
            "Train Epoch:8[19200/60000 32%]\tTrain Loss:0.114149\n",
            "Train Epoch:8[25600/60000 43%]\tTrain Loss:0.331623\n",
            "Train Epoch:8[32000/60000 53%]\tTrain Loss:0.073089\n",
            "Train Epoch:8[38400/60000 64%]\tTrain Loss:0.182846\n",
            "Train Epoch:8[44800/60000 75%]\tTrain Loss:0.015920\n",
            "Train Epoch:8[51200/60000 85%]\tTrain Loss:0.187800\n",
            "Train Epoch:8[57600/60000 96%]\tTrain Loss:0.069101\n",
            "\n",
            "[EPOCH:8]\\ \tTest Loss : 0.0030 \tTest Accuracy:96.95%\n",
            "\n",
            "Train Epoch:9[0/60000 0%]\tTrain Loss:0.044018\n",
            "Train Epoch:9[6400/60000 11%]\tTrain Loss:0.049929\n",
            "Train Epoch:9[12800/60000 21%]\tTrain Loss:0.070538\n",
            "Train Epoch:9[19200/60000 32%]\tTrain Loss:0.094354\n",
            "Train Epoch:9[25600/60000 43%]\tTrain Loss:0.079536\n",
            "Train Epoch:9[32000/60000 53%]\tTrain Loss:0.040476\n",
            "Train Epoch:9[38400/60000 64%]\tTrain Loss:0.041143\n",
            "Train Epoch:9[44800/60000 75%]\tTrain Loss:0.042622\n",
            "Train Epoch:9[51200/60000 85%]\tTrain Loss:0.045737\n",
            "Train Epoch:9[57600/60000 96%]\tTrain Loss:0.075826\n",
            "\n",
            "[EPOCH:9]\\ \tTest Loss : 0.0027 \tTest Accuracy:97.32%\n",
            "\n",
            "Train Epoch:10[0/60000 0%]\tTrain Loss:0.098554\n",
            "Train Epoch:10[6400/60000 11%]\tTrain Loss:0.053207\n",
            "Train Epoch:10[12800/60000 21%]\tTrain Loss:0.026369\n",
            "Train Epoch:10[19200/60000 32%]\tTrain Loss:0.074704\n",
            "Train Epoch:10[25600/60000 43%]\tTrain Loss:0.055265\n",
            "Train Epoch:10[32000/60000 53%]\tTrain Loss:0.048704\n",
            "Train Epoch:10[38400/60000 64%]\tTrain Loss:0.031953\n",
            "Train Epoch:10[44800/60000 75%]\tTrain Loss:0.021942\n",
            "Train Epoch:10[51200/60000 85%]\tTrain Loss:0.064979\n",
            "Train Epoch:10[57600/60000 96%]\tTrain Loss:0.184110\n",
            "\n",
            "[EPOCH:10]\\ \tTest Loss : 0.0027 \tTest Accuracy:97.40%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKWRoxmJ4F7Z"
      },
      "source": [
        "---\n",
        "### Autograd를 사용\n",
        "\n",
        "- Pytorch의 자동미분 엔진\n",
        "\t- 신경망이 BackPropagation(역전파)를 사용할 때,\n",
        "\t- 파라미터 업데이트 하는 방법을 구현하기 위한 엔진\n",
        "\n",
        "- 4가지 값을 이용\n",
        "\t- BATSH_SIZE = 64\n",
        "\t- INPUT_SIZE = 1000\n",
        "\t- HIDDEN_SIZE = 100\n",
        "\t- OUTPUT_SIZE = 10\n",
        "\n",
        "        - INPUT_SIZE가 (64, 1000)인 데이터와\n",
        "        - (1000, 100) 크기의 행렬과 행렬곱을 계산하기 위한 수\n",
        "        - OUTPUT_SIZE는 최종으로 출력되는 값의 벡터의 크기를 의미"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYc1b-JqRp9J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}